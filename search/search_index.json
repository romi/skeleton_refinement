{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Skeleton refinement","text":"<p>For full documentation of the ROMI project visit docs.romi-project.eu.</p>"},{"location":"#about","title":"About","text":"<p>This library is intended to provide the implementation of a skeleton refinement method published here:</p> <p>Chaudhury A. and Godin C. (2020) Skeletonization of Plant Point Cloud Data Using Stochastic Optimization Framework. Front. Plant Sci. 11:773. doi: 10.3389/fpls.2020.00773.</p> <p></p> <p>This is a part of the implementation of the stochastic registration algorithm based on the following paper: Myronenko A. and Song X. (2010) Point set registration: Coherent Point drift. IEEE Transactions on Pattern Analysis and Machine Intelligence. 32 (2): 2262-2275. DOI: 10.1109/TPAMI.2010.46</p>"},{"location":"#installation","title":"Installation","text":"<p>We strongly advise to create isolated environments to install the ROMI libraries.</p> <p>We often use <code>conda</code> as an environment and python package manager. If you do not yet have <code>miniconda3</code> installed on your system, have a look here.</p> <p>The <code>skeleton_refinement</code> package is available from the <code>romi-eu</code> channel.</p>"},{"location":"#existing-conda-environment","title":"Existing conda environment","text":"<p>To install the <code>skeleton_refinement</code> conda package in an existing environment, first activate it, then proceed as follows: <pre><code>conda install skeleton_refinement -c romi-eu\n</code></pre></p>"},{"location":"#new-conda-environment","title":"New conda environment","text":"<p>To install the <code>skeleton_refinement</code> conda package in a new environment, here named <code>romi</code>, proceed as follows: <pre><code>conda create -n romi skeleton_refinement -c romi-eu\n</code></pre></p>"},{"location":"#installation-from-sources","title":"Installation from sources","text":"<p>To install this library, simply clone the repo and use <code>pip</code> to install it and the required dependencies. Again, we strongly advise to create a <code>conda</code> environment.</p> <p>All this can be done as follows: <pre><code>git clone https://github.com/romi/skeleton_refinement.git\ncd skeleton_refinement\nconda create -n skeleton_refinement 'python =3.10'\nconda activate skeleton_refinement  # do not forget to activate your environment!\npython -m pip install -e .  # install the sources\n</code></pre></p> <p>Note that the <code>-e</code> option is to install the <code>skeleton_refinement</code> sources in \"developer mode\". That is, if you make changes to the source code of <code>skeleton_refinement</code> you will not have to <code>pip install</code> it again.</p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#example-dataset","title":"Example dataset","text":"<p>First, we download an example dataset from Zenodo, named <code>real_plant_analyzed</code>, to play with:</p> <pre><code>wget https://zenodo.org/records/10379172/files/real_plant_analyzed.zip\nunzip real_plant_analyzed.zip\n</code></pre> <p>It contains:   * a plant point cloud under <code>PointCloud_1_0_1_0_10_0_7ee836e5a9/PointCloud.ply</code>   * a plant skeleton under <code>CurveSkeleton__TriangleMesh_0393cb5708/CurveSkeleton.json</code></p>"},{"location":"#cli","title":"CLI","text":"<p>You may use the <code>refine_skeleton</code> CLI to refine a given skeleton using the original point cloud: </p> <pre><code>cd real_plant_analyzed\nrefine_skeleton \\\n  PointCloud_1_0_1_0_10_0_7ee836e5a9/PointCloud.ply \\\n  CurveSkeleton__TriangleMesh_0393cb5708/CurveSkeleton.json \\\n  optimized_skeleton.txt\n</code></pre>"},{"location":"#python-api","title":"Python API","text":"<p>Here is a minimal example how to use the <code>skeleton_refinement</code> library in Python:</p> <pre><code>from skeleton_refinement.stochastic_registration import perform_registration\nfrom skeleton_refinement.io import load_json, load_ply\n\npcd = load_ply(\"real_plant_analyzed/PointCloud_1_0_1_0_10_0_7ee836e5a9/PointCloud.ply\")\nskel = load_json(\"real_plant_analyzed/CurveSkeleton__TriangleMesh_0393cb5708/CurveSkeleton.json\", \"points\")\n# Perform stochastic optimization\nrefined_skel = perform_registration(pcd, skel)\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\nax.scatter(*pcd.T, marker='.', color='black')\nax.scatter(*skel.T, marker='o', color='r')\nax.scatter(*refined_skel.T, marker='o', color='b')\nax.set_aspect('equal')\nplt.show()\n</code></pre>"},{"location":"dev/","title":"Developers &amp; contributors","text":""},{"location":"dev/#unitary-tests","title":"Unitary tests","text":"<p>Some tests are defined in the <code>tests</code> directory. We use <code>nose2</code> to call them as follows:</p> <pre><code>nose2 -v -C\n</code></pre> <p>Notes:</p> <ul> <li>the configuration file used by <code>nose2</code> is <code>unittests.cfg</code></li> <li>the <code>-C</code> option generate a coverage report, as defined by the <code>.coveragerc</code> file.</li> <li>this requires the <code>nose2</code> &amp; <code>coverage</code> packages listed in the <code>requirements.txt</code> file.</li> </ul> <p>You first have to install the library from sources as explained here.</p>"},{"location":"dev/#conda-packaging","title":"Conda packaging","text":"<p>Start by installing the required <code>conda-build</code> &amp; <code>anaconda-client</code> conda packages in the <code>base</code> environment as follows: <pre><code>conda install -n base conda-build anaconda-client\n</code></pre></p>"},{"location":"dev/#build-a-conda-package","title":"Build a conda package","text":"<p>To build the <code>romitask</code> conda package, from the root directory of the repository and the <code>base</code> conda environment, run: <pre><code>conda build conda/recipe/ -c conda-forge --user romi-eu\n</code></pre></p> <p>If you are struggling with some of the modifications you made to the recipe,  notably when using environment variables or Jinja2 stuffs, you can always render the recipe with: <pre><code>conda render conda/recipe/\n</code></pre></p> <p>The official documentation for <code>conda-render</code> can be found here.</p>"},{"location":"dev/#upload-a-conda-package","title":"Upload a conda package","text":"<p>To upload the built package, you need a valid account (here <code>romi-eu</code>) on anaconda.org &amp; to log ONCE with <code>anaconda login</code>, then: <pre><code>anaconda upload ~/miniconda3/conda-bld/linux-64/skeleton_refinement*.tar.bz2 --user romi-eu\n</code></pre></p>"},{"location":"dev/#clean-builds","title":"Clean builds","text":"<p>To clean the source and build intermediates: <pre><code>conda build purge\n</code></pre></p> <p>To clean ALL the built packages &amp; build environments: <pre><code>conda build purge-all\n</code></pre></p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>skeleton_refinement<ul> <li>cli<ul> <li>refine_skeleton</li> </ul> </li> <li>deformable_registration</li> <li>expectation_maximization_registration</li> <li>io</li> <li>stochastic_registration</li> <li>utilities</li> </ul> </li> </ul>"},{"location":"reference/skeleton_refinement/","title":"skeleton_refinement","text":""},{"location":"reference/skeleton_refinement/deformable_registration/","title":"deformable_registration","text":""},{"location":"reference/skeleton_refinement/deformable_registration/#skeleton_refinement.deformable_registration--deformable-registration","title":"Deformable Registration","text":"<p>This module provides a non-rigid point set registration implementation using the Coherent Point Drift (CPD) algorithm, enabling accurate alignment of point clouds with non-linear deformations.</p>"},{"location":"reference/skeleton_refinement/deformable_registration/#skeleton_refinement.deformable_registration--key-features","title":"Key Features","text":"<ul> <li>Non-rigid point cloud alignment using Gaussian Mixture Models (GMM)</li> <li>Coherent Point Drift algorithm implementation with customizable parameters</li> <li>Efficient transformation calculation with regularization for smooth deformations</li> <li>Support for arbitrary dimensional point clouds</li> <li>Built on a generic Expectation-Maximization registration framework</li> </ul>"},{"location":"reference/skeleton_refinement/deformable_registration/#skeleton_refinement.deformable_registration--notes","title":"Notes","text":"<p>This is a part of the implementation of the stochastic registration algorithm based on the following paper: Myronenko A. and Song X. (2010) Point set registration: Coherent Point drift. IEEE Transactions on Pattern Analysis and Machine Intelligence. 32 (2): 2262-2275. DOI: 10.1109/TPAMI.2010.46</p> <p>The library is based on the python implementation of the paper in <code>pycpd</code> package.</p>"},{"location":"reference/skeleton_refinement/deformable_registration/#skeleton_refinement.deformable_registration--usage-examples","title":"Usage Examples","text":"<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from skeleton_refinement.deformable_registration import DeformableRegistration\n&gt;&gt;&gt; # Create sample point sets\n&gt;&gt;&gt; X = np.random.rand(10, 3)  # Reference point set\n&gt;&gt;&gt; Y = np.random.rand(10, 3)  # Point set to be aligned\n&gt;&gt;&gt; # Initialize and run registration\n&gt;&gt;&gt; reg = DeformableRegistration(X=X, Y=Y, alpha=2, beta=2)\n&gt;&gt;&gt; TY = reg.register()\n&gt;&gt;&gt; # Get registration parameters\n&gt;&gt;&gt; G, W = reg.get_registration_parameters()\n</code></pre>"},{"location":"reference/skeleton_refinement/deformable_registration/#skeleton_refinement.deformable_registration.DeformableRegistration","title":"DeformableRegistration","text":"<pre><code>DeformableRegistration(alpha=ALPHA, beta=BETA, *args, **kwargs)\n</code></pre> <p>               Bases: <code>ExpectationMaximizationRegistration</code></p> <p>Deformable point set registration using Coherent Point Drift algorithm.</p> <p>This class implements the non-rigid point set registration algorithm from the paper: \"Point Set Registration: Coherent Point Drift\" by Myronenko and Song (2010). It optimizes a Gaussian Mixture Model (GMM) to find correspondences between two point sets and computes a non-rigid transformation.</p> <p>Attributes:</p> Name Type Description <code>alpha</code> <code>float</code> <p>Regularization weight controlling the smoothness of deformation.</p> <code>beta</code> <code>float</code> <p>Width of Gaussian kernel used in the non-rigid transformation.</p> <code>W</code> <code>ndarray</code> <p>Deformation matrix of shape <code>(M, D)</code> where <code>M</code> is the number of points in <code>Y</code> and <code>D</code> is the dimension.</p> <code>G</code> <code>ndarray</code> <p>Gaussian kernel matrix of shape <code>(M, M)</code>, computed from <code>Y</code> using <code>beta</code> as width.</p> <code>TY</code> <code>ndarray</code> <p>The transformed point set <code>Y</code> after registration.</p> <code>sigma2</code> <code>float</code> <p>Final variance of GMM.</p> Notes <p>The implementation uses Expectation-Maximization algorithm to optimize the transformation. The non-rigid transformation is represented as <code>T(Y) = Y + G*W</code> where <code>G</code> is the Gaussian kernel and <code>W</code> is optimized.</p> See Also <p>skeleton_refinement.expectation_maximization_registration.ExpectationMaximizationRegistration : Base class for EM-based registration algorithms.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from skeleton_refinement.deformable_registration import DeformableRegistration\n&gt;&gt;&gt; # Create sample point sets\n&gt;&gt;&gt; X = np.random.rand(10, 3)  # Reference point set\n&gt;&gt;&gt; Y = np.random.rand(10, 3)  # Point set to be aligned\n&gt;&gt;&gt; # Initialize and run registration\n&gt;&gt;&gt; reg = DeformableRegistration(X=X, Y=Y, alpha=2, beta=2)\n&gt;&gt;&gt; TY = reg.register()\n&gt;&gt;&gt; # Get registration parameters\n&gt;&gt;&gt; G, W = reg.get_registration_parameters()\n</code></pre> <p>Initialize the deformable registration algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>Regularization weight controlling the smoothness of deformation. Higher values result in smoother deformation. Default is <code>2</code>.</p> <code>ALPHA</code> <code>beta</code> <code>float</code> <p>Width of Gaussian kernel used in the non-rigid transformation. Controls the interaction between points. Default is <code>2</code>.</p> <code>BETA</code> <code>X</code> <code>ndarray</code> <p>Reference point set of shape <code>(N, D)</code> where <code>N</code> is number of points and <code>D</code> is dimension.</p> required <code>Y</code> <code>ndarray</code> <p>Point set to be aligned to <code>X</code>, of shape <code>(M, D)</code> where <code>M</code> is number of points.</p> required <code>sigma2</code> <code>float</code> <p>Initial variance of GMM. If <code>None</code>, it's computed from data.</p> required <code>max_iterations</code> <code>int</code> <p>Maximum number of iterations for the optimization algorithm.</p> required <code>tolerance</code> <code>float</code> <p>Convergence threshold based on change in <code>sigma2</code>.</p> required <code>w</code> <code>float</code> <p>Weight of the uniform distribution component, range <code>[0,1]</code>. Used to account for outliers. Default is <code>0</code>.</p> required Source code in <code>skeleton_refinement/deformable_registration.py</code> <pre><code>def __init__(self, alpha=ALPHA, beta=BETA, *args, **kwargs):\n    \"\"\"Initialize the deformable registration algorithm.\n\n    Parameters\n    ----------\n    alpha : float, optional\n        Regularization weight controlling the smoothness of deformation.\n        Higher values result in smoother deformation. Default is ``2``.\n    beta : float, optional\n        Width of Gaussian kernel used in the non-rigid transformation.\n        Controls the interaction between points. Default is ``2``.\n    X : numpy.ndarray\n        Reference point set of shape ``(N, D)`` where ``N`` is number of points and ``D`` is dimension.\n    Y : numpy.ndarray\n        Point set to be aligned to ``X``, of shape ``(M, D)`` where ``M`` is number of points.\n    sigma2 : float, optional\n        Initial variance of GMM. If ``None``, it's computed from data.\n    max_iterations : int, optional\n        Maximum number of iterations for the optimization algorithm.\n    tolerance : float, optional\n        Convergence threshold based on change in `sigma2`.\n    w : float, optional\n        Weight of the uniform distribution component, range ``[0,1]``.\n        Used to account for outliers. Default is ``0``.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.alpha = ALPHA if alpha is None else alpha\n    self.beta = BETA if alpha is None else beta\n    self.W = np.zeros((self.M, self.D))\n    self.G = gaussian_kernel(self.Y, self.beta)\n</code></pre>"},{"location":"reference/skeleton_refinement/deformable_registration/#skeleton_refinement.deformable_registration.DeformableRegistration.get_registration_parameters","title":"get_registration_parameters","text":"<pre><code>get_registration_parameters()\n</code></pre> <p>Retrieve the registration parameters <code>G</code> &amp; <code>W</code>.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Gaussian kernel matrix of shape <code>(M, M)</code>.</p> <code>ndarray</code> <p>Deformation matrix of shape <code>(M, D)</code>.</p> Notes <p>These parameters can be used to apply the learned transformation to other point sets using the formula: <code>Y_transformed = Y + G*W</code></p> Source code in <code>skeleton_refinement/deformable_registration.py</code> <pre><code>def get_registration_parameters(self):\n    \"\"\"Retrieve the registration parameters `G` &amp; `W`.\n\n    Returns\n    -------\n    numpy.ndarray\n        Gaussian kernel matrix of shape ``(M, M)``.\n    numpy.ndarray\n        Deformation matrix of shape ``(M, D)``.\n\n    Notes\n    -----\n    These parameters can be used to apply the learned transformation\n    to other point sets using the formula: ``Y_transformed = Y + G*W``\n    \"\"\"\n    return self.G, self.W\n</code></pre>"},{"location":"reference/skeleton_refinement/deformable_registration/#skeleton_refinement.deformable_registration.DeformableRegistration.transform_point_cloud","title":"transform_point_cloud","text":"<pre><code>transform_point_cloud(Y=None)\n</code></pre> <p>Apply the non-rigid transformation to a point cloud.</p> <p>The transformation is defined as: <code>T(Y) = Y + G*W</code>, where <code>G</code> is the Gaussian kernel and <code>W</code> is the deformation matrix.</p> <p>Parameters:</p> Name Type Description Default <code>Y</code> <code>ndarray</code> <p>Point cloud to transform of shape <code>(M, D)</code>. If <code>None</code>, transforms the stored point cloud <code>self.Y</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray or None</code> <p>Transformed point cloud of the same shape as input <code>Y</code>. If <code>Y</code> is <code>None</code>, updates <code>self.TY</code> and returns <code>None</code>.</p> Source code in <code>skeleton_refinement/deformable_registration.py</code> <pre><code>def transform_point_cloud(self, Y=None):\n    \"\"\"Apply the non-rigid transformation to a point cloud.\n\n    The transformation is defined as: ``T(Y) = Y + G*W``,\n    where ``G`` is the Gaussian kernel and ``W`` is the deformation matrix.\n\n    Parameters\n    ----------\n    Y : numpy.ndarray, optional\n        Point cloud to transform of shape ``(M, D)``.\n        If `None`, transforms the stored point cloud ``self.Y``.\n\n    Returns\n    -------\n    numpy.ndarray or None\n        Transformed point cloud of the same shape as input ``Y``.\n        If ``Y`` is ``None``, updates ``self.TY`` and returns `None`.\n    \"\"\"\n    if Y is None:\n        # Apply non-rigid transformation to the class's own point cloud\n        # TY = Y + G*W where G is the Gaussian kernel matrix and W is the deformation matrix\n        self.TY = self.Y + np.dot(self.G, self.W)\n        return\n    else:\n        # Apply transformation to the input point cloud and return the result\n        # Returns the transformed points without modifying internal state\n        return Y + np.dot(self.G, self.W)\n</code></pre>"},{"location":"reference/skeleton_refinement/deformable_registration/#skeleton_refinement.deformable_registration.DeformableRegistration.update_transform","title":"update_transform","text":"<pre><code>update_transform()\n</code></pre> <p>Update the transformation parameters.</p> <p>Solves for the deformation matrix W that minimizes the energy function. This is computed by solving the linear system: <code>(DP1*G + alpha*sigma2*I)*W = P*X - DP1*Y</code>, where:</p> <ul> <li><code>DP1</code> is a diagonal matrix with elements of <code>P1</code>,</li> <li><code>G</code> is the Gaussian kernel,</li> <li><code>I</code> is the identity matrix,</li> <li><code>P</code> is the posterior probability matrix.</li> </ul> Source code in <code>skeleton_refinement/deformable_registration.py</code> <pre><code>def update_transform(self):\n    \"\"\"Update the transformation parameters.\n\n    Solves for the deformation matrix W that minimizes the energy function.\n    This is computed by solving the linear system: ``(DP1*G + alpha*sigma2*I)*W = P*X - DP1*Y``, where:\n\n      - ``DP1`` is a diagonal matrix with elements of ``P1``,\n      - ``G`` is the Gaussian kernel,\n      - ``I`` is the identity matrix,\n      - ``P`` is the posterior probability matrix.\n    \"\"\"\n    # Solve for optimal deformation matrix W in CPD algorithm\n    # A: Left side of linear equation system combining point correspondences and regularization\n    A = np.dot(np.diag(self.P1), self.G) + self.alpha * self.sigma2 * np.eye(self.M)  # P1-weighted kernel matrix + regularization term\n\n    # B: Right side of equation system representing the difference between points\n    B = np.dot(self.P, self.X) - np.dot(np.diag(self.P1), self.Y)  # P-weighted X points minus P1-weighted Y points\n\n    # Compute deformation matrix W by solving linear system AW = B\n    self.W = np.linalg.solve(A, B)  # W determines how points in Y are transformed\n</code></pre>"},{"location":"reference/skeleton_refinement/deformable_registration/#skeleton_refinement.deformable_registration.DeformableRegistration.update_variance","title":"update_variance","text":"<pre><code>update_variance()\n</code></pre> <p>Update the variance (<code>sigma2</code>) of the Gaussian Mixture Model.</p> <p>Computes the weighted distance between the transformed <code>Y</code> (<code>TY</code>) and the reference point cloud <code>X</code>, normalized by the number of points and dimensions. The updated variance is used to evaluate convergence in the EM algorithm.</p> Source code in <code>skeleton_refinement/deformable_registration.py</code> <pre><code>def update_variance(self):\n    \"\"\"Update the variance (``sigma2``) of the Gaussian Mixture Model.\n\n    Computes the weighted distance between the transformed ``Y`` (``TY``) and the\n    reference point cloud ``X``, normalized by the number of points and dimensions.\n    The updated variance is used to evaluate convergence in the EM algorithm.\n    \"\"\"\n    # Store previous sigma2 value to calculate change later\n    qprev = self.sigma2\n\n    # Calculate weighted sum of squared norms of X points: P^T * (X^2)\n    xPx = np.dot(np.transpose(self.Pt1), np.sum(np.multiply(self.X, self.X), axis=1))\n    # Calculate weighted sum of squared norms of transformed Y points: P1^T * (TY^2)\n    yPy = np.dot(np.transpose(self.P1), np.sum(np.multiply(self.TY, self.TY), axis=1))\n    # Calculate trace of P * X * Y^T (cross-correlation term)\n    trPXY = np.sum(np.multiply(self.TY, np.dot(self.P, self.X)))\n\n    # Update sigma2 using the formula from CPD algorithm:\n    # \u03c3\u00b2 = (xPx - 2*trPXY + yPy) / (Np * D)\n    # where Np is number of points and D is dimensionality\n    self.sigma2 = (xPx - 2 * trPXY + yPy) / (self.Np * self.D)\n\n    # Prevent numerical issues by setting a minimum threshold for sigma2\n    if self.sigma2 &lt;= 0:\n        self.sigma2 = self.tolerance / 10\n\n    # Calculate absolute change in sigma2 for convergence check\n    self.err = np.abs(self.sigma2 - qprev)\n</code></pre>"},{"location":"reference/skeleton_refinement/expectation_maximization_registration/","title":"expectation_maximization_registration","text":""},{"location":"reference/skeleton_refinement/expectation_maximization_registration/#skeleton_refinement.expectation_maximization_registration--point-cloud-registration-using-expectation-maximization","title":"Point Cloud Registration using Expectation-Maximization","text":"<p>This module implements the Expectation-Maximization algorithm for point cloud registration, providing a probabilistic approach to align 3D point sets with robust handling of noise and outliers. This is an abstract base class that should be implemented by specific registration methods.</p>"},{"location":"reference/skeleton_refinement/expectation_maximization_registration/#skeleton_refinement.expectation_maximization_registration--key-features","title":"Key Features","text":"<ul> <li>Probabilistic point cloud alignment using EM algorithm</li> <li>Iterative refinement of transformation parameters</li> <li>Automatic variance estimation for noise handling</li> <li>Support for rigid and non-rigid transformations</li> <li>Convergence control through iteration limits and tolerance settings</li> </ul>"},{"location":"reference/skeleton_refinement/expectation_maximization_registration/#skeleton_refinement.expectation_maximization_registration--notes","title":"Notes","text":"<p>This is a part of the implementation of the stochastic registration algorithm based on the following paper: Myronenko A. and Song X. (2010) Point set registration: Coherent Point drift. IEEE Transactions on Pattern Analysis and Machine Intelligence. 32 (2): 2262-2275. DOI: 10.1109/TPAMI.2010.46</p> <p>The library is based on the python implementation of the paper in <code>pycpd</code> package.</p>"},{"location":"reference/skeleton_refinement/expectation_maximization_registration/#skeleton_refinement.expectation_maximization_registration.ExpectationMaximizationRegistration","title":"ExpectationMaximizationRegistration","text":"<pre><code>ExpectationMaximizationRegistration(X, Y, sigma2=None, max_iterations=MAX_ITER, tolerance=TOL, w=0, *args, **kwargs)\n</code></pre> <p>               Bases: <code>object</code></p> <p>Abstract base class for point cloud registration using Expectation-Maximization algorithm.</p> <p>This class implements the core functionality of the Coherent Point Drift (CPD) algorithm for point set registration based on Myronenko and Song's paper. It uses a probabilistic approach where the alignment of two point sets is treated as a Maximum Likelihood (ML) estimation problem with a Gaussian Mixture Model (GMM) as the likelihood function.</p> <p>The class serves as a base for various CPD registration methods (rigid, affine, etc.), providing common EM framework while requiring specific transformation models to be implemented in child classes.</p> <p>Attributes:</p> Name Type Description <code>X</code> <code>ndarray</code> <p>Reference point cloud coordinates, shape <code>(N, D)</code>.</p> <code>Y</code> <code>ndarray</code> <p>Initial point cloud coordinates to optimize, shape <code>(M, D)</code>.</p> <code>TY</code> <code>ndarray</code> <p>Transformed/registered version of Y after optimization, shape <code>(M, D)</code>.</p> <code>sigma2</code> <code>float</code> <p>Variance of the Gaussian Mixture Model (GMM), updated during registration.</p> <code>N</code> <code>int</code> <p>Number of points in reference cloud <code>X</code>.</p> <code>M</code> <code>int</code> <p>Number of points in source cloud <code>Y</code>.</p> <code>D</code> <code>int</code> <p>Dimensionality of the point clouds (e.g., 3 for 3D point clouds).</p> <code>tolerance</code> <code>float</code> <p>Convergence criterion threshold.</p> <code>w</code> <code>float</code> <p>Weight of the uniform distribution component for outlier handling.</p> <code>max_iterations</code> <code>int</code> <p>Maximum number of iterations for the algorithm.</p> <code>iteration</code> <code>int</code> <p>Current iteration number during registration process.</p> <code>err</code> <code>float</code> <p>Current registration error/distance between point sets.</p> <code>P</code> <code>ndarray</code> <p>Posterior probability matrix of point correspondences, shape <code>(M, N)</code>.</p> <code>Pt1</code> <code>ndarray</code> <p>Column-wise sum of posterior probability matrix, shape <code>(N,)</code>.</p> <code>P1</code> <code>ndarray</code> <p>Row-wise sum of posterior probability matrix, shape <code>(M,)</code>.</p> <code>Np</code> <code>float</code> <p>Sum of all elements in the posterior probability matrix.</p> <code>q</code> <code>float</code> <p>Negative log-likelihood of the current estimate.</p> Notes <p>This is an abstract base class. Child classes must implement:</p> <ul> <li><code>update_transform()</code>: Update transformation parameters</li> <li><code>transform_point_cloud()</code>: Apply transformation to point cloud</li> <li><code>update_variance()</code>: Update GMM variance</li> <li><code>get_registration_parameters()</code>: Return registration parameters</li> </ul> References <p>Myronenko A. and Song X. (2010) Point set registration: Coherent Point drift. IEEE Transactions on Pattern Analysis and Machine Intelligence. 32 (2): 2262-2275. DOI: 10.1109/TPAMI.2010.46</p> See Also <p>skeleton_refinement.utilities.initialize_sigma2 : Function to initialize the variance parameter</p> <p>Initialize the Expectation-Maximization registration algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Reference point cloud (target), shape <code>(N, D)</code>.</p> required <code>Y</code> <code>ndarray</code> <p>Point cloud to be aligned (source), shape <code>(M, D)</code>.</p> required <code>sigma2</code> <code>float or None</code> <p>Initial variance of the Gaussian Mixture Model (GMM). If <code>None</code>, it will be estimated from data. Default is <code>None</code>.</p> <code>None</code> <code>max_iterations</code> <code>int</code> <p>Maximum number of EM iterations. Default is <code>100</code>.</p> <code>MAX_ITER</code> <code>tolerance</code> <code>float</code> <p>Convergence threshold for stopping iterations. Algorithm stops when change in error falls below this value. Default is <code>0.0001</code>.</p> <code>TOL</code> <code>w</code> <code>float</code> <p>Weight of the uniform distribution component (0 &lt;= w &lt; 1). Used to account for outliers and noise. A value of <code>0</code> means no outlier handling. Default is <code>0</code>.</p> <code>0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>X</code> or <code>Y</code> are not 2D numpy arrays, or if their dimensions don't match.</p> Source code in <code>skeleton_refinement/expectation_maximization_registration.py</code> <pre><code>def __init__(self, X, Y, sigma2=None, max_iterations=MAX_ITER, tolerance=TOL, w=0, *args, **kwargs):\n    \"\"\"Initialize the Expectation-Maximization registration algorithm.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        Reference point cloud (target), shape ``(N, D)``.\n    Y : numpy.ndarray\n        Point cloud to be aligned (source), shape ``(M, D)``.\n    sigma2 : float or None, optional\n        Initial variance of the Gaussian Mixture Model (GMM).\n        If ``None``, it will be estimated from data.\n        Default is ``None``.\n    max_iterations : int, optional\n        Maximum number of EM iterations. Default is ``100``.\n    tolerance : float, optional\n        Convergence threshold for stopping iterations.\n        Algorithm stops when change in error falls below this value.\n        Default is ``0.0001``.\n    w : float, optional\n        Weight of the uniform distribution component (0 &lt;= w &lt; 1).\n        Used to account for outliers and noise.\n        A value of ``0`` means no outlier handling.\n        Default is ``0``.\n\n    Raises\n    ------\n    ValueError\n        If `X` or `Y` are not 2D numpy arrays, or if their dimensions don't match.\n    \"\"\"\n    if not isinstance(X, np.ndarray) or X.ndim != 2:\n        raise ValueError(\"The target point cloud (X) must be at a 2D numpy array.\")\n    if not isinstance(Y, np.ndarray) or Y.ndim != 2:\n        raise ValueError(\"The source point cloud (Y) must be a 2D numpy array.\")\n    if X.shape[1] != Y.shape[1]:\n        raise ValueError(\"Both point clouds need to have the same number of dimensions.\")\n\n    self.X = X\n    self.Y = Y\n    self.sigma2 = sigma2\n    (self.N, self.D) = self.X.shape\n    (self.M, _) = self.Y.shape\n    self.tolerance = tolerance\n    self.w = w\n    self.max_iterations = max_iterations\n    self.iteration = 0\n    self.err = self.tolerance + 1\n    self.P = np.zeros((self.M, self.N))\n    self.Pt1 = np.zeros((self.N,))\n    self.P1 = np.zeros((self.M,))\n    self.Np = 0\n\n    self.TY = None\n</code></pre>"},{"location":"reference/skeleton_refinement/expectation_maximization_registration/#skeleton_refinement.expectation_maximization_registration.ExpectationMaximizationRegistration.expectation","title":"expectation","text":"<pre><code>expectation()\n</code></pre> <p>Perform the Expectation step of the EM algorithm.</p> <p>The expectation step estimates the posterior probability (P) that each point in the source set corresponds to each point in the reference set, based on the current transformation and GMM variance.</p> <p>This step also handles outlier detection based on the uniform distribution weight parameter w.</p> Notes <p>Updates the following attributes:</p> <ul> <li>P: Posterior probability matrix of point correspondences</li> <li>Pt1: Column-wise sum of P</li> <li>P1: Row-wise sum of P</li> <li>Np: Sum of all elements in P</li> </ul> Source code in <code>skeleton_refinement/expectation_maximization_registration.py</code> <pre><code>def expectation(self):\n    \"\"\"Perform the Expectation step of the EM algorithm.\n\n    The expectation step estimates the posterior probability (P) that each\n    point in the source set corresponds to each point in the reference set,\n    based on the current transformation and GMM variance.\n\n    This step also handles outlier detection based on the uniform distribution\n    weight parameter w.\n\n    Notes\n    -----\n    Updates the following attributes:\n\n    - P: Posterior probability matrix of point correspondences\n    - Pt1: Column-wise sum of P\n    - P1: Row-wise sum of P\n    - Np: Sum of all elements in P\n    \"\"\"\n    # Initialize posterior probability matrix (M source points \u00d7 N reference points)\n    P = np.zeros((self.M, self.N))\n\n    # Calculate squared Mahalanobis distances between transformed source points and reference points\n    for i in range(0, self.M):\n        # Calculate differences between current transformed point and all reference points\n        diff = self.X - np.tile(self.TY[i, :], (self.N, 1))\n        # Square the differences\n        diff = np.multiply(diff, diff)\n        # Sum squared differences across dimensions for each point pair\n        P[i, :] = P[i, :] + np.sum(diff, axis=1)\n\n    # Calculate uniform distribution component for outlier handling\n    c = (2 * np.pi * self.sigma2) ** (self.D / 2)  # Normalization factor for Gaussian\n    c = c * self.w / (1 - self.w)  # Scale by outlier ratio\n    c = c * self.M / self.N  # Normalize by point cloud sizes\n\n    # Convert distances to probabilities using Gaussian kernel\n    P = np.exp(-P / (2 * self.sigma2))\n\n    # Calculate denominator for posterior probability normalization\n    den = np.sum(P, axis=0)\n    den = np.tile(den, (self.M, 1))\n    # Avoid division by zero\n    den[den == 0] = np.finfo(float).eps\n    # Add uniform component for outlier handling\n    den += c\n\n    # Compute normalized posterior probabilities\n    self.P = np.divide(P, den)\n\n    # Calculate marginal probabilities and total correspondence strength\n    self.Pt1 = np.sum(self.P, axis=0)  # Column-wise sum - probability mass for each reference point\n    self.P1 = np.sum(self.P, axis=1)  # Row-wise sum - probability mass for each source point\n    self.Np = np.sum(self.P1)  # Total correspondence probability mass\n</code></pre>"},{"location":"reference/skeleton_refinement/expectation_maximization_registration/#skeleton_refinement.expectation_maximization_registration.ExpectationMaximizationRegistration.get_registration_parameters","title":"get_registration_parameters","text":"<pre><code>get_registration_parameters()\n</code></pre> <p>Get the current registration transformation parameters.</p> <p>This is an abstract method that must be implemented by child classes to return the specific transformation parameters used in the registration.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the transformation parameters specific to the registration method.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If called from the base class without being overridden.</p> Source code in <code>skeleton_refinement/expectation_maximization_registration.py</code> <pre><code>def get_registration_parameters(self):\n    \"\"\"Get the current registration transformation parameters.\n\n    This is an abstract method that must be implemented by child classes\n    to return the specific transformation parameters used in the registration.\n\n    Returns\n    -------\n    dict\n        Dictionary containing the transformation parameters specific to\n        the registration method.\n\n    Raises\n    ------\n    NotImplementedError\n        If called from the base class without being overridden.\n    \"\"\"\n    raise NotImplementedError(\"Registration parameters should be defined in child classes.\")\n</code></pre>"},{"location":"reference/skeleton_refinement/expectation_maximization_registration/#skeleton_refinement.expectation_maximization_registration.ExpectationMaximizationRegistration.iterate","title":"iterate","text":"<pre><code>iterate()\n</code></pre> <p>Perform one Expectation-Maximization iteration.</p> <p>This method runs a single EM iteration consisting of:</p> <ol> <li>Expectation step: compute point correspondences</li> <li>Maximization step: update transformation parameters</li> </ol> <p>The iteration counter is incremented after each call.</p> Source code in <code>skeleton_refinement/expectation_maximization_registration.py</code> <pre><code>def iterate(self):\n    \"\"\"Perform one Expectation-Maximization iteration.\n\n    This method runs a single EM iteration consisting of:\n\n    1. Expectation step: compute point correspondences\n    2. Maximization step: update transformation parameters\n\n    The iteration counter is incremented after each call.\n    \"\"\"\n    self.expectation()\n    self.maximization()\n    self.iteration += 1\n</code></pre>"},{"location":"reference/skeleton_refinement/expectation_maximization_registration/#skeleton_refinement.expectation_maximization_registration.ExpectationMaximizationRegistration.maximization","title":"maximization","text":"<pre><code>maximization()\n</code></pre> <p>Perform the Maximization step of the EM algorithm.</p> <p>The maximization step updates the transformation parameters and variance to maximize the probability that the transformed source points were drawn from the GMM centered at the reference points.</p> <p>This method calls the abstract methods that should be implemented by child classes:</p> <ol> <li>update_transform()</li> <li>transform_point_cloud()</li> <li>update_variance()</li> </ol> Source code in <code>skeleton_refinement/expectation_maximization_registration.py</code> <pre><code>def maximization(self):\n    \"\"\"Perform the Maximization step of the EM algorithm.\n\n    The maximization step updates the transformation parameters and variance\n    to maximize the probability that the transformed source points were drawn\n    from the GMM centered at the reference points.\n\n    This method calls the abstract methods that should be implemented by child classes:\n\n    1. update_transform()\n    2. transform_point_cloud()\n    3. update_variance()\n    \"\"\"\n    self.update_transform()\n    self.transform_point_cloud()\n    self.update_variance()\n</code></pre>"},{"location":"reference/skeleton_refinement/expectation_maximization_registration/#skeleton_refinement.expectation_maximization_registration.ExpectationMaximizationRegistration.register","title":"register","text":"<pre><code>register(callback=lambda **kwargs: None)\n</code></pre> <p>Perform the point set registration.</p> <p>This method runs the EM algorithm to align the source point cloud (Y) to the reference point cloud (X). The algorithm iteratively estimates point correspondences and updates the transformation parameters until convergence or maximum iterations are reached.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>callable</code> <p>Function to call after each iteration with registration state information. The function should accept keyword arguments: iteration, error, X, Y. Default is a no-op function.</p> <code>lambda **kwargs: None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The transformed point cloud (TY).</p> <code>dict</code> <p>Registration parameters specific to the registration method.</p> Notes <p>The registration is considered converged when the change in error between iterations falls below the tolerance threshold or the maximum number of iterations is reached.</p> Source code in <code>skeleton_refinement/expectation_maximization_registration.py</code> <pre><code>def register(self, callback=lambda **kwargs: None):\n    \"\"\"Perform the point set registration.\n\n    This method runs the EM algorithm to align the source point cloud (Y)\n    to the reference point cloud (X). The algorithm iteratively estimates\n    point correspondences and updates the transformation parameters until\n    convergence or maximum iterations are reached.\n\n    Parameters\n    ----------\n    callback : callable, optional\n        Function to call after each iteration with registration state information.\n        The function should accept keyword arguments: iteration, error, X, Y.\n        Default is a no-op function.\n\n    Returns\n    -------\n    numpy.ndarray\n        The transformed point cloud (TY).\n    dict\n        Registration parameters specific to the registration method.\n\n    Notes\n    -----\n    The registration is considered converged when the change in error between\n    iterations falls below the tolerance threshold or the maximum number of\n    iterations is reached.\n    \"\"\"\n    # Initialize by transforming points according to current parameters\n    self.transform_point_cloud()\n\n    # If variance is not provided, calculate initial variance based on point clouds\n    if self.sigma2 is None:\n        self.sigma2 = initialize_sigma2(self.X, self.TY)\n\n    # Initialize negative log-likelihood (q) based on current error and variance\n    self.q = -self.err - self.N * self.D / 2 * np.log(self.sigma2)\n\n    # Create progress bar\n    pbar = tqdm(total=self.max_iterations, desc=\"Registration\")\n\n    # Main EM loop - continue until convergence or max iterations\n    while self.iteration &lt; self.max_iterations and self.err &gt; self.tolerance:\n        # Run one iteration of Expectation-Maximization algorithm\n        self.iterate()\n        # If callback is provided, execute it with current registration state\n        if callable(callback):\n            kwargs = {'iteration': self.iteration, 'error': self.err, 'X': self.X, 'Y': self.TY}\n            callback(**kwargs)\n        # Update progress bar\n        pbar.update(1)\n        pbar.set_postfix({\"error\": f\"{self.err:.6f}\", 'tol.': f'{self.tolerance}'})\n        # If we've reached convergence, update to max to close the bar\n        if self.err &lt;= self.tolerance:\n            pbar.n = self.max_iterations\n            pbar.set_postfix({\"error\": f\"{self.err:.6f}/{self.tolerance}\", \"total n_iter\": f\"{self.iteration}\"})\n            pbar.refresh()\n\n    # Close the progress bar\n    pbar.close()\n    return\n</code></pre>"},{"location":"reference/skeleton_refinement/expectation_maximization_registration/#skeleton_refinement.expectation_maximization_registration.ExpectationMaximizationRegistration.transform_point_cloud","title":"transform_point_cloud","text":"<pre><code>transform_point_cloud()\n</code></pre> <p>Apply the current transformation to the source point cloud.</p> <p>This is an abstract method that must be implemented by child classes to apply the specific transformation to the point cloud Y and update TY.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If called from the base class without being overridden.</p> Source code in <code>skeleton_refinement/expectation_maximization_registration.py</code> <pre><code>def transform_point_cloud(self):\n    \"\"\"Apply the current transformation to the source point cloud.\n\n    This is an abstract method that must be implemented by child classes\n    to apply the specific transformation to the point cloud Y and update TY.\n\n    Raises\n    ------\n    NotImplementedError\n        If called from the base class without being overridden.\n    \"\"\"\n    raise NotImplementedError(\"This method should be defined in child classes.\")\n</code></pre>"},{"location":"reference/skeleton_refinement/expectation_maximization_registration/#skeleton_refinement.expectation_maximization_registration.ExpectationMaximizationRegistration.update_transform","title":"update_transform","text":"<pre><code>update_transform()\n</code></pre> <p>Update transformation parameters based on the current point correspondence.</p> <p>This is an abstract method that must be implemented by child classes to update the specific transformation parameters (e.g., rotation matrix, scaling factor, etc.) based on the current state of the registration.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If called from the base class without being overridden.</p> Source code in <code>skeleton_refinement/expectation_maximization_registration.py</code> <pre><code>def update_transform(self):\n    \"\"\"Update transformation parameters based on the current point correspondence.\n\n    This is an abstract method that must be implemented by child classes\n    to update the specific transformation parameters (e.g., rotation matrix,\n    scaling factor, etc.) based on the current state of the registration.\n\n    Raises\n    ------\n    NotImplementedError\n        If called from the base class without being overridden.\n    \"\"\"\n    raise NotImplementedError(\"This method should be defined in child classes.\")\n</code></pre>"},{"location":"reference/skeleton_refinement/expectation_maximization_registration/#skeleton_refinement.expectation_maximization_registration.ExpectationMaximizationRegistration.update_variance","title":"update_variance","text":"<pre><code>update_variance()\n</code></pre> <p>Update the variance of the GMM model (sigma2).</p> <p>This is an abstract method that must be implemented by child classes to update the variance parameter based on the current state of the registration process.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If called from the base class without being overridden.</p> Source code in <code>skeleton_refinement/expectation_maximization_registration.py</code> <pre><code>def update_variance(self):\n    \"\"\"Update the variance of the GMM model (sigma2).\n\n    This is an abstract method that must be implemented by child classes\n    to update the variance parameter based on the current state of the\n    registration process.\n\n    Raises\n    ------\n    NotImplementedError\n        If called from the base class without being overridden.\n    \"\"\"\n    raise NotImplementedError(\"This method should be defined in child classes.\")\n</code></pre>"},{"location":"reference/skeleton_refinement/io/","title":"io","text":""},{"location":"reference/skeleton_refinement/io/#skeleton_refinement.io--3d-skeleton-and-point-cloud-io","title":"3D Skeleton and Point Cloud I/O","text":"<p>This module provides functions for loading and saving 3D point clouds and skeleton data in various file formats, simplifying data interchange between different tools and libraries.</p>"},{"location":"reference/skeleton_refinement/io/#skeleton_refinement.io--key-features","title":"Key Features","text":"<ul> <li>Load point clouds from XYZ, PLY and JSON formats</li> <li>Load skeleton data from NetworkX graph files</li> <li>Save tree structures to JSON and NetworkX pickle formats</li> <li>Support for various coordinate formats and attribute handling</li> </ul>"},{"location":"reference/skeleton_refinement/io/#skeleton_refinement.io.load_json","title":"load_json","text":"<pre><code>load_json(filename, key=None)\n</code></pre> <p>Load a point cloud or skeleton file from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str or Path</code> <p>Path to the JSON file to parse.</p> required <code>key</code> <code>str</code> <p>The key of the JSON dictionary containing the point cloud or skeleton coordinates to load. If <code>None</code>, the entire JSON content is returned.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The XYZ coordinates of the point cloud or skeleton as a NumPy array with shape <code>(n, 3)</code>, where <code>n</code> is the number of points.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from skeleton_refinement.io import load_json\n&gt;&gt;&gt; # JSON file with structure: {\"points\": [[x1,y1,z1], [x2,y2,z2], ...]}\n&gt;&gt;&gt; points = load_json('point_cloud.json', key='points')\n&gt;&gt;&gt; print(points.shape)\n(1000, 3)\n&gt;&gt;&gt; # JSON file with direct array: [[x1,y1,z1], [x2,y2,z2], ...]\n&gt;&gt;&gt; points = load_json('simple_points.json')\n&gt;&gt;&gt; print(points.shape)\n(1000, 3)\n</code></pre> Source code in <code>skeleton_refinement/io.py</code> <pre><code>def load_json(filename, key=None):\n    \"\"\"Load a point cloud or skeleton file from a JSON file.\n\n    Parameters\n    ----------\n    filename : str or pathlib.Path\n        Path to the JSON file to parse.\n    key : str, optional\n        The key of the JSON dictionary containing the point cloud or skeleton\n        coordinates to load. If ``None``, the entire JSON content is returned.\n\n    Returns\n    -------\n    numpy.ndarray\n        The XYZ coordinates of the point cloud or skeleton as a NumPy array\n        with shape ``(n, 3)``, where ``n`` is the number of points.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from skeleton_refinement.io import load_json\n    &gt;&gt;&gt; # JSON file with structure: {\"points\": [[x1,y1,z1], [x2,y2,z2], ...]}\n    &gt;&gt;&gt; points = load_json('point_cloud.json', key='points')\n    &gt;&gt;&gt; print(points.shape)\n    (1000, 3)\n    &gt;&gt;&gt; # JSON file with direct array: [[x1,y1,z1], [x2,y2,z2], ...]\n    &gt;&gt;&gt; points = load_json('simple_points.json')\n    &gt;&gt;&gt; print(points.shape)\n    (1000, 3)\n    \"\"\"\n    import json\n    with open(filename, mode='rb') as f:\n        X = json.load(f)\n\n    if key is not None:\n        X = X[key]\n    return np.array(X)\n</code></pre>"},{"location":"reference/skeleton_refinement/io/#skeleton_refinement.io.load_nx","title":"load_nx","text":"<pre><code>load_nx(filename, key='position')\n</code></pre> <p>Load a tree graph from a pickled NetworkX file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str or Path</code> <p>Path to the pickled NetworkX graph file to parse.</p> required <code>key</code> <code>str</code> <p>The node attribute key containing the position data. Default is 'position'.</p> <code>'position'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The XYZ coordinates of the nodes as a NumPy array with shape (n, 3), where n is the number of nodes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from skeleton_refinement.io import load_nx\n&gt;&gt;&gt; positions = load_nx('graph.pkl')\n&gt;&gt;&gt; print(positions.shape)\n(50, 3)\n&gt;&gt;&gt; # Load custom attribute\n&gt;&gt;&gt; attributes = load_nx('graph.pkl', key='custom_attribute')\n</code></pre> Notes <p>The NetworkX graph must have nodes with the specified attribute.</p> Source code in <code>skeleton_refinement/io.py</code> <pre><code>def load_nx(filename, key='position'):\n    \"\"\"Load a tree graph from a pickled NetworkX file.\n\n    Parameters\n    ----------\n    filename : str or pathlib.Path\n        Path to the pickled NetworkX graph file to parse.\n    key : str, optional\n        The node attribute key containing the position data.\n        Default is 'position'.\n\n    Returns\n    -------\n    numpy.ndarray\n        The XYZ coordinates of the nodes as a NumPy array with shape (n, 3),\n        where n is the number of nodes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from skeleton_refinement.io import load_nx\n    &gt;&gt;&gt; positions = load_nx('graph.pkl')\n    &gt;&gt;&gt; print(positions.shape)\n    (50, 3)\n    &gt;&gt;&gt; # Load custom attribute\n    &gt;&gt;&gt; attributes = load_nx('graph.pkl', key='custom_attribute')\n\n    Notes\n    -----\n    The NetworkX graph must have nodes with the specified attribute.\n    \"\"\"\n    import pickle\n    with open(filename, mode='rb') as f:\n        G = pickle.load(f)\n\n    X = []\n    for node in G.nodes:\n        X.append(G.nodes[node][key])\n\n    return np.array(X)\n</code></pre>"},{"location":"reference/skeleton_refinement/io/#skeleton_refinement.io.load_ply","title":"load_ply","text":"<pre><code>load_ply(filename)\n</code></pre> <p>Load point cloud coordinates from a PLY file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str or Path</code> <p>Path to the PLY file to parse.</p> required <p>Returns:</p> Type Description <code>The XYZ coordinates of the point cloud as a NumPy array</code> <p>with shape <code>(n, 3)</code>, where <code>n</code> is the number of points.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from skeleton_refinement.io import load_ply\n&gt;&gt;&gt; points = load_ply('point_cloud.ply')\n&gt;&gt;&gt; print(points.shape)\n(1000, 3)\n&gt;&gt;&gt; print(points[:2])\n[[ 1.2  3.4  5.6]\n [-0.1  0.2  0.3]]\n</code></pre> Source code in <code>skeleton_refinement/io.py</code> <pre><code>def load_ply(filename):\n    \"\"\"Load point cloud coordinates from a PLY file.\n\n    Parameters\n    ----------\n    filename : str or pathlib.Path\n        Path to the PLY file to parse.\n\n    Returns\n    -------\n    The XYZ coordinates of the point cloud as a NumPy array\n        with shape ``(n, 3)``, where ``n`` is the number of points.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from skeleton_refinement.io import load_ply\n    &gt;&gt;&gt; points = load_ply('point_cloud.ply')\n    &gt;&gt;&gt; print(points.shape)\n    (1000, 3)\n    &gt;&gt;&gt; print(points[:2])\n    [[ 1.2  3.4  5.6]\n     [-0.1  0.2  0.3]]\n    \"\"\"\n    from plyfile import PlyData\n    plydata = PlyData.read(filename)\n    X = np.array([plydata['vertex']['x'], plydata['vertex']['y'], plydata['vertex']['z']]).T\n    return X\n</code></pre>"},{"location":"reference/skeleton_refinement/io/#skeleton_refinement.io.load_xyz","title":"load_xyz","text":"<pre><code>load_xyz(filename)\n</code></pre> <p>Load a point cloud or skeleton file saved as a series of space-separated XYZ coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str or Path</code> <p>Path to the point cloud or skeleton file to parse.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The XYZ coordinates of the point cloud or skeleton as a NumPy array with shape <code>(n, 3)</code>, where <code>n</code> is the number of points.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from skeleton_refinement.io import load_xyz\n&gt;&gt;&gt; points = load_xyz('point_cloud.xyz')\n&gt;&gt;&gt; print(points.shape)\n(1000, 3)\n&gt;&gt;&gt; print(points[:2])\n[[ 1.2  3.4  5.6]\n [-0.1  0.2  0.3]]\n</code></pre> Source code in <code>skeleton_refinement/io.py</code> <pre><code>def load_xyz(filename):\n    \"\"\"Load a point cloud or skeleton file saved as a series of space-separated XYZ coordinates.\n\n    Parameters\n    ----------\n    filename : str or pathlib.Path\n        Path to the point cloud or skeleton file to parse.\n\n    Returns\n    -------\n    numpy.ndarray\n        The XYZ coordinates of the point cloud or skeleton as a NumPy array\n        with shape ``(n, 3)``, where ``n`` is the number of points.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from skeleton_refinement.io import load_xyz\n    &gt;&gt;&gt; points = load_xyz('point_cloud.xyz')\n    &gt;&gt;&gt; print(points.shape)\n    (1000, 3)\n    &gt;&gt;&gt; print(points[:2])\n    [[ 1.2  3.4  5.6]\n     [-0.1  0.2  0.3]]\n    \"\"\"\n    f = open(filename, \"r\")\n    lines = f.readlines()\n    org_x = []\n    org_y = []\n    org_z = []\n    for l in lines:\n        org_x.append(float(l.split(' ')[0]))\n        org_y.append(float(l.split(' ')[1]))\n        org_z.append(float(l.split(' ')[2]))\n    f.close()\n    X = np.column_stack((org_x, org_y, org_z))\n    return X\n</code></pre>"},{"location":"reference/skeleton_refinement/io/#skeleton_refinement.io.save_json","title":"save_json","text":"<pre><code>save_json(filename, G, **kwargs)\n</code></pre> <p>Save a tree graph to a JSON file.</p> <p>This function exports a NetworkX graph to a JSON file format, storing both node positions and edge connections.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str or Path</code> <p>Path to the JSON file to write.</p> required <code>G</code> <code>Graph</code> <p>Graph to write. Nodes must have a 'position' attribute.</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to <code>json.dumps()</code>. If 'indent' is not provided, it defaults to <code>2</code>.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import networkx as nx\n&gt;&gt;&gt; from skeleton_refinement.io import save_json\n&gt;&gt;&gt; G = nx.Graph()\n&gt;&gt;&gt; G.add_node(0, position=[0,0,0])\n&gt;&gt;&gt; G.add_node(1, position=[1,1,1])\n&gt;&gt;&gt; G.add_edge(0, 1)\n&gt;&gt;&gt; save_json('graph.json', G)\n&gt;&gt;&gt; # With custom JSON formatting\n&gt;&gt;&gt; save_json('pretty_graph.json', G, indent=4, sort_keys=True)\n</code></pre> Notes <p>The output JSON structure will contain: - 'points': list of node positions - 'lines': list of edges</p> Source code in <code>skeleton_refinement/io.py</code> <pre><code>def save_json(filename, G, **kwargs):\n    \"\"\"Save a tree graph to a JSON file.\n\n    This function exports a NetworkX graph to a JSON file format,\n    storing both node positions and edge connections.\n\n    Parameters\n    ----------\n    filename : str or pathlib.Path\n        Path to the JSON file to write.\n    G : networkx.Graph\n        Graph to write. Nodes must have a 'position' attribute.\n    **kwargs\n        Additional keyword arguments to pass to ``json.dumps()``.\n        If 'indent' is not provided, it defaults to ``2``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import networkx as nx\n    &gt;&gt;&gt; from skeleton_refinement.io import save_json\n    &gt;&gt;&gt; G = nx.Graph()\n    &gt;&gt;&gt; G.add_node(0, position=[0,0,0])\n    &gt;&gt;&gt; G.add_node(1, position=[1,1,1])\n    &gt;&gt;&gt; G.add_edge(0, 1)\n    &gt;&gt;&gt; save_json('graph.json', G)\n    &gt;&gt;&gt; # With custom JSON formatting\n    &gt;&gt;&gt; save_json('pretty_graph.json', G, indent=4, sort_keys=True)\n\n    Notes\n    -----\n    The output JSON structure will contain:\n    - 'points': list of node positions\n    - 'lines': list of edges\n    \"\"\"\n    import json\n    data = {\n        \"points\": [G.nodes[node]['position'] for node in G.nodes],\n        \"lines\": list(G.edges),\n    }\n    if 'indent' not in kwargs:\n        kwargs.update({'indent': 2})\n    with open(filename, 'w') as f:\n        f.writelines(json.dumps(data, **kwargs))\n    return\n</code></pre>"},{"location":"reference/skeleton_refinement/io/#skeleton_refinement.io.save_nx","title":"save_nx","text":"<pre><code>save_nx(filename, G, **kwargs)\n</code></pre> <p>Save a tree graph to a pickle file.</p> <p>This function saves a NetworkX graph to a pickle file for later retrieval.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str or Path</code> <p>Path to the pickle file to write.</p> required <code>G</code> <code>Graph</code> <p>Graph to write.</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to pickle.dump(). If 'protocol' is not provided, it defaults to pickle.HIGHEST_PROTOCOL.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import networkx as nx\n&gt;&gt;&gt; from skeleton_refinement.io import save_nx\n&gt;&gt;&gt; G = nx.Graph()\n&gt;&gt;&gt; G.add_node(0, position=[0,0,0])\n&gt;&gt;&gt; G.add_node(1, position=[1,1,1])\n&gt;&gt;&gt; G.add_edge(0, 1)\n&gt;&gt;&gt; save_nx('graph.pkl', G)\n&gt;&gt;&gt; # With specific protocol\n&gt;&gt;&gt; save_nx('graph_v2.pkl', G, protocol=2)\n</code></pre> Notes <p>The pickle format is not secure against erroneous or maliciously constructed data. Never unpickle data received from untrusted or unauthenticated sources.</p> Source code in <code>skeleton_refinement/io.py</code> <pre><code>def save_nx(filename, G, **kwargs):\n    \"\"\"Save a tree graph to a pickle file.\n\n    This function saves a NetworkX graph to a pickle file for later retrieval.\n\n    Parameters\n    ----------\n    filename : str or pathlib.Path\n        Path to the pickle file to write.\n    G : networkx.Graph\n        Graph to write.\n    **kwargs\n        Additional keyword arguments to pass to pickle.dump().\n        If 'protocol' is not provided, it defaults to pickle.HIGHEST_PROTOCOL.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import networkx as nx\n    &gt;&gt;&gt; from skeleton_refinement.io import save_nx\n    &gt;&gt;&gt; G = nx.Graph()\n    &gt;&gt;&gt; G.add_node(0, position=[0,0,0])\n    &gt;&gt;&gt; G.add_node(1, position=[1,1,1])\n    &gt;&gt;&gt; G.add_edge(0, 1)\n    &gt;&gt;&gt; save_nx('graph.pkl', G)\n    &gt;&gt;&gt; # With specific protocol\n    &gt;&gt;&gt; save_nx('graph_v2.pkl', G, protocol=2)\n\n    Notes\n    -----\n    The pickle format is not secure against erroneous or maliciously constructed data.\n    Never unpickle data received from untrusted or unauthenticated sources.\n    \"\"\"\n    import pickle\n    if 'protocol' not in kwargs:\n        kwargs['protocol'] = pickle.HIGHEST_PROTOCOL\n    if isinstance(filename, str):\n        filename = Path(filename)\n    with filename.open(mode='wb') as f:\n        pickle.dump(G, f, **kwargs)\n    return\n</code></pre>"},{"location":"reference/skeleton_refinement/stochastic_registration/","title":"stochastic_registration","text":""},{"location":"reference/skeleton_refinement/stochastic_registration/#skeleton_refinement.stochastic_registration--stochastic-point-cloud-registration","title":"Stochastic Point Cloud Registration","text":"<p>A module for aligning and registering 3D skeleton structures to point clouds using stochastic optimization techniques. It enables precise fitting of skeletal models to noisy or dense point cloud data, commonly used in 3D plant structure analysis.</p>"},{"location":"reference/skeleton_refinement/stochastic_registration/#skeleton_refinement.stochastic_registration--key-features","title":"Key Features","text":"<ul> <li>Non-rigid point set registration using Coherent Point Drift (CPD) algorithm</li> <li>Optimized skeleton alignment to underlying point cloud structures</li> <li>Minimum spanning tree construction from point sets using k-nearest neighbors</li> <li>Configurable regularization parameters for controlling deformation smoothness</li> <li>Support for handling outliers and varying point densities</li> </ul>"},{"location":"reference/skeleton_refinement/stochastic_registration/#skeleton_refinement.stochastic_registration--usage-examples","title":"Usage Examples","text":"<pre><code>&gt;&gt;&gt; from skeleton_refinement.stochastic_registration import perform_registration, knn_mst\n&gt;&gt;&gt; from skeleton_refinement.io import load_ply, load_json\n&gt;&gt;&gt; # Load point cloud and skeleton data\n&gt;&gt;&gt; pcd = load_ply(\"path/to/pointcloud.ply\")\n&gt;&gt;&gt; skel = load_json(\"path/to/skeleton.json\", \"points\")\n&gt;&gt;&gt; # Perform registration to align skeleton with point cloud\n&gt;&gt;&gt; refined_skel = perform_registration(pcd, skel, alpha=5, beta=5)\n&gt;&gt;&gt; # Generate skeleton graph structure\n&gt;&gt;&gt; skel_tree = knn_mst(refined_skel)\n</code></pre>"},{"location":"reference/skeleton_refinement/stochastic_registration/#skeleton_refinement.stochastic_registration.knn_mst","title":"knn_mst","text":"<pre><code>knn_mst(skeleton_points, n_neighbors=5, knn_algorithm='kd_tree', mst_algorithm='kruskal')\n</code></pre> <p>Create a minimum spanning tree from skeleton points using k-nearest neighbors graph.</p> <p>This function constructs a k-nearest neighbors graph from the input skeleton points using Euclidean distances, then computes the minimum spanning tree of this graph. The resulting tree represents the skeleton structure as a connected graph with minimal total edge weight.</p> <p>Parameters:</p> Name Type Description Default <code>skeleton_points</code> <code>ndarray</code> <p>The skeleton coordinates of shape <code>(n_points, 3)</code>, with XYZ coordinates. Each row represents the 3D position of a skeleton point.</p> required <code>n_neighbors</code> <code>int</code> <p>The number of neighbors to consider for each point when building the k-nearest neighbors graph. Default is <code>5</code>.</p> <code>5</code> <code>knn_algorithm</code> <code>(auto, ball_tree, kd_tree, brute)</code> <p>The algorithm to use for computing the k-nearest neighbors. - 'kd_tree': KD Tree algorithm, works well for low dimensions. - 'ball_tree': Ball Tree algorithm, works well for high dimensions. - 'brute': Brute force algorithm, always uses all data points. - 'auto': Automatically chooses the most appropriate algorithm. Default is <code>'kd_tree'</code>.</p> <code>'auto'</code> <code>mst_algorithm</code> <code>(kruskal, prim, boruvka)</code> <p>The algorithm to use for computing the minimum spanning tree. - 'kruskal': Kruskal's algorithm, efficient for sparse graphs. - 'prim': Prim's algorithm, efficient for dense graphs. - 'boruvka': Bor\u016fvka's algorithm, another MST algorithm. Default is <code>'kruskal'</code>.</p> <code>'kruskal'</code> <p>Returns:</p> Type Description <code>Graph</code> <p>A NetworkX Graph representing the skeleton structure as a minimum spanning tree. - Nodes correspond to skeleton points with their 3D coordinates stored as a 'position' attribute - Edges connect points that are part of the minimum spanning tree - Edge weights represent Euclidean distances between connected points</p> Notes <p>The function first builds a k-nearest neighbors graph where each point is connected to its k nearest neighbors. Then it computes the minimum spanning tree of this graph to create a connected structure with minimal total edge length.</p> <p>The Minkowski distance with <code>p=2</code> is used, which corresponds to the Euclidean distance.</p> See Also <p>sklearn.neighbors.NearestNeighbors : For finding nearest neighbors networkx.minimum_spanning_tree : For computing minimum spanning trees</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from skeleton_refinement.stochastic_registration import perform_registration\n&gt;&gt;&gt; from skeleton_refinement.stochastic_registration import knn_mst\n&gt;&gt;&gt; from skeleton_refinement.io import load_ply, load_json\n&gt;&gt;&gt; pcd = load_ply(\"real_plant_analyzed/PointCloud_1_0_1_0_10_0_7ee836e5a9/PointCloud.ply\")\n&gt;&gt;&gt; skel = load_json(\"real_plant_analyzed/CurveSkeleton__TriangleMesh_0393cb5708/CurveSkeleton.json\", \"points\")\n&gt;&gt;&gt; # Perform stochastic optimization\n&gt;&gt;&gt; refined_skel = perform_registration(pcd, skel, alpha=5, beta=5)\n&gt;&gt;&gt; # Compute skeleton tree structure using mst on knn-graph:\n&gt;&gt;&gt; skel_tree = knn_mst(refined_skel)\n&gt;&gt;&gt; # The output is a NetworkX Graph with nodes representing skeleton points\n&gt;&gt;&gt; print(f\"Number of nodes: {skel_tree.number_of_nodes()}\")\n&gt;&gt;&gt; print(f\"Number of edges: {skel_tree.number_of_edges()}\")\n&gt;&gt;&gt; # Access node coordinates\n&gt;&gt;&gt; sample_node = list(skel_tree.nodes())[0]\n&gt;&gt;&gt; print(f\"Position of node {sample_node}: {skel_tree.nodes[sample_node]['position']}\")\n</code></pre> Source code in <code>skeleton_refinement/stochastic_registration.py</code> <pre><code>def knn_mst(skeleton_points, n_neighbors=5, knn_algorithm='kd_tree', mst_algorithm='kruskal'):\n    \"\"\"Create a minimum spanning tree from skeleton points using k-nearest neighbors graph.\n\n    This function constructs a k-nearest neighbors graph from the input skeleton points\n    using Euclidean distances, then computes the minimum spanning tree of this graph.\n    The resulting tree represents the skeleton structure as a connected graph with\n    minimal total edge weight.\n\n    Parameters\n    ----------\n    skeleton_points : numpy.ndarray\n        The skeleton coordinates of shape ``(n_points, 3)``, with XYZ coordinates.\n        Each row represents the 3D position of a skeleton point.\n    n_neighbors : int, optional\n        The number of neighbors to consider for each point when building the\n        k-nearest neighbors graph. Default is ``5``.\n    knn_algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n        The algorithm to use for computing the k-nearest neighbors.\n        - 'kd_tree': KD Tree algorithm, works well for low dimensions.\n        - 'ball_tree': Ball Tree algorithm, works well for high dimensions.\n        - 'brute': Brute force algorithm, always uses all data points.\n        - 'auto': Automatically chooses the most appropriate algorithm.\n        Default is `'kd_tree'`.\n    mst_algorithm : {'kruskal', 'prim', 'boruvka'}, optional\n        The algorithm to use for computing the minimum spanning tree.\n        - 'kruskal': Kruskal's algorithm, efficient for sparse graphs.\n        - 'prim': Prim's algorithm, efficient for dense graphs.\n        - 'boruvka': Bor\u016fvka's algorithm, another MST algorithm.\n        Default is `'kruskal'`.\n\n    Returns\n    -------\n    networkx.Graph\n        A NetworkX Graph representing the skeleton structure as a minimum spanning tree.\n        - Nodes correspond to skeleton points with their 3D coordinates stored as a 'position' attribute\n        - Edges connect points that are part of the minimum spanning tree\n        - Edge weights represent Euclidean distances between connected points\n\n    Notes\n    -----\n    The function first builds a k-nearest neighbors graph where each point is connected\n    to its k nearest neighbors. Then it computes the minimum spanning tree of this graph\n    to create a connected structure with minimal total edge length.\n\n    The Minkowski distance with ``p=2`` is used, which corresponds to the Euclidean distance.\n\n    See Also\n    --------\n    sklearn.neighbors.NearestNeighbors : For finding nearest neighbors\n    networkx.minimum_spanning_tree : For computing minimum spanning trees\n\n    Examples\n    --------\n    &gt;&gt;&gt; from skeleton_refinement.stochastic_registration import perform_registration\n    &gt;&gt;&gt; from skeleton_refinement.stochastic_registration import knn_mst\n    &gt;&gt;&gt; from skeleton_refinement.io import load_ply, load_json\n    &gt;&gt;&gt; pcd = load_ply(\"real_plant_analyzed/PointCloud_1_0_1_0_10_0_7ee836e5a9/PointCloud.ply\")\n    &gt;&gt;&gt; skel = load_json(\"real_plant_analyzed/CurveSkeleton__TriangleMesh_0393cb5708/CurveSkeleton.json\", \"points\")\n    &gt;&gt;&gt; # Perform stochastic optimization\n    &gt;&gt;&gt; refined_skel = perform_registration(pcd, skel, alpha=5, beta=5)\n    &gt;&gt;&gt; # Compute skeleton tree structure using mst on knn-graph:\n    &gt;&gt;&gt; skel_tree = knn_mst(refined_skel)\n    &gt;&gt;&gt; # The output is a NetworkX Graph with nodes representing skeleton points\n    &gt;&gt;&gt; print(f\"Number of nodes: {skel_tree.number_of_nodes()}\")\n    &gt;&gt;&gt; print(f\"Number of edges: {skel_tree.number_of_edges()}\")\n    &gt;&gt;&gt; # Access node coordinates\n    &gt;&gt;&gt; sample_node = list(skel_tree.nodes())[0]\n    &gt;&gt;&gt; print(f\"Position of node {sample_node}: {skel_tree.nodes[sample_node]['position']}\")\n    \"\"\"\n    # Initialize nearest neighbors model with Euclidean distance (Minkowski p=2)\n    nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=knn_algorithm, metric=\"minkowski\", p=2).fit(\n        skeleton_points)\n    # Find k-nearest neighbors for each point - returns distances and indices matrices\n    distances, indices = nbrs.kneighbors(skeleton_points)\n\n    # Initialize empty undirected graph to build KNN representation\n    G = nx.Graph()\n\n    # Add edges between each point and its k-nearest neighbors\n    for row, nodes_idx in enumerate(indices):\n        nodes_idx = list(map(int, nodes_idx))\n        node_idx, nei_idx = nodes_idx[0], nodes_idx[1:]  # First index is the point itself\n        # Add edges with Euclidean distances as weights\n        # Skip first neighbor (index 0) as it's the point itself\n        [G.add_edges_from([(node_idx, n_idx, {\"weight\": distances[row, col + 1]})]) for col, n_idx in\n         enumerate(nei_idx)]\n\n    # Store original 3D coordinates as node attributes\n    for node_id in G.nodes:\n        G.nodes[node_id]['position'] = skeleton_points[node_id]\n\n    # Compute minimum spanning tree from the KNN graph\n    # This creates optimal connected structure with minimal total edge length\n    T = nx.minimum_spanning_tree(G, algorithm=mst_algorithm)\n\n    return T\n</code></pre>"},{"location":"reference/skeleton_refinement/stochastic_registration/#skeleton_refinement.stochastic_registration.perform_registration","title":"perform_registration","text":"<pre><code>perform_registration(X, Y, **kwargs)\n</code></pre> <p>Performs stochastic deformation registration to align a skeleton to a point cloud.</p> <p>This function uses the Coherent Point Drift (CPD) algorithm to perform non-rigid registration between a reference point cloud and a skeleton. The algorithm optimizes the skeleton's position to better align with the underlying point cloud structure.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input reference point cloud coordinates of shape <code>(n_points, dim)</code>, XYZ sorted. This represents the fixed point set that the skeleton will be aligned to.</p> required <code>Y</code> <code>ndarray</code> <p>The input reference skeleton coordinates of shape <code>(n_points, dim)</code>, XYZ sorted. This represents the moving point set that will be transformed.</p> required <p>Other Parameters:</p> Name Type Description <code>alpha</code> <code>float</code> <p>Regularization weight that controls the smoothness of deformation. Higher values result in smoother, more rigid transformations.</p> <code>beta</code> <code>float</code> <p>Width of the Gaussian kernel used in the non-rigid transformation. Controls the influence range of each point in the deformation.</p> <code>sigma2</code> <code>float or ndarray</code> <p>Initial variance of the Gaussian Mixture Model. If None, it will be estimated automatically from the point sets. Defaults to <code>None</code>.</p> <code>max_iterations</code> <code>int</code> <p>The maximum number of iterations before stopping the iterative registration. Defaults to <code>100</code>.</p> <code>tolerance</code> <code>float</code> <p>Convergence criterion. The algorithm stops when the error falls below this value. Defaults to <code>0.001</code>.</p> <code>w</code> <code>float</code> <p>Weight of the uniform distribution in the mixture model. Used to account for outliers. Value between 0 and 1. Defaults to <code>0</code>.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>The transformed skeleton coordinates of shape <code>(n_points, dim)</code>, XYZ sorted. This is the optimized skeleton aligned to the input point cloud.</p> Notes <p>The function internally uses the DeformableRegistration class which implements the Coherent Point Drift algorithm. The registration process involves an Expectation-Maximization approach to optimize the transformation parameters.</p> <p>The parameters alpha and beta are crucial for obtaining good results: - Higher alpha values enforce more rigidity in the transformation - Beta determines the spatial extent of interaction between points</p> See Also <p>skeleton_refinement.deformable_registration.DeformableRegistration : Class that implements the CPD algorithm. skeleton_refinement.expectation_maximization_registration.ExpectationMaximizationRegistration : Base class for EM-based registration.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from skeleton_refinement.stochastic_registration import perform_registration\n&gt;&gt;&gt; from skeleton_refinement.io import load_ply, load_json\n&gt;&gt;&gt; pcd = load_ply(\"tests/testdata/real_plant_analyzed/PointCloud_1_0_1_0_10_0_7ee836e5a9/PointCloud.ply\")\n&gt;&gt;&gt; skel = load_json(\"tests/testdata/real_plant_analyzed/CurveSkeleton__TriangleMesh_0393cb5708/CurveSkeleton.json\", \"points\")\n&gt;&gt;&gt; # Perform stochastic optimization\n&gt;&gt;&gt; refined_skel = perform_registration(pcd, skel, alpha=5, beta=5)\n&gt;&gt;&gt; print(skel.shape)\n</code></pre> Source code in <code>skeleton_refinement/stochastic_registration.py</code> <pre><code>def perform_registration(X, Y, **kwargs):\n    \"\"\"Performs stochastic deformation registration to align a skeleton to a point cloud.\n\n    This function uses the Coherent Point Drift (CPD) algorithm to perform non-rigid\n    registration between a reference point cloud and a skeleton. The algorithm optimizes\n    the skeleton's position to better align with the underlying point cloud structure.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        The input reference point cloud coordinates of shape `(n_points, dim)`, XYZ sorted.\n        This represents the fixed point set that the skeleton will be aligned to.\n    Y : numpy.ndarray\n        The input reference skeleton coordinates of shape `(n_points, dim)`, XYZ sorted.\n        This represents the moving point set that will be transformed.\n\n    Other Parameters\n    ----------------\n    alpha : float\n        Regularization weight that controls the smoothness of deformation.\n        Higher values result in smoother, more rigid transformations.\n    beta : float\n        Width of the Gaussian kernel used in the non-rigid transformation.\n        Controls the influence range of each point in the deformation.\n    sigma2 : float or numpy.ndarray, optional\n        Initial variance of the Gaussian Mixture Model.\n        If None, it will be estimated automatically from the point sets.\n        Defaults to ``None``.\n    max_iterations : int, optional\n        The maximum number of iterations before stopping the iterative registration.\n        Defaults to ``100``.\n    tolerance : float, optional\n        Convergence criterion. The algorithm stops when the error falls below this value.\n        Defaults to ``0.001``.\n    w : float, optional\n        Weight of the uniform distribution in the mixture model.\n        Used to account for outliers. Value between 0 and 1.\n        Defaults to ``0``.\n\n    Returns\n    -------\n    numpy.ndarray\n        The transformed skeleton coordinates of shape `(n_points, dim)`, XYZ sorted.\n        This is the optimized skeleton aligned to the input point cloud.\n\n    Notes\n    -----\n    The function internally uses the DeformableRegistration class which implements\n    the Coherent Point Drift algorithm. The registration process involves an\n    Expectation-Maximization approach to optimize the transformation parameters.\n\n    The parameters alpha and beta are crucial for obtaining good results:\n    - Higher alpha values enforce more rigidity in the transformation\n    - Beta determines the spatial extent of interaction between points\n\n    See Also\n    --------\n    skeleton_refinement.deformable_registration.DeformableRegistration : Class that implements the CPD algorithm.\n    skeleton_refinement.expectation_maximization_registration.ExpectationMaximizationRegistration : Base class for EM-based registration.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from skeleton_refinement.stochastic_registration import perform_registration\n    &gt;&gt;&gt; from skeleton_refinement.io import load_ply, load_json\n    &gt;&gt;&gt; pcd = load_ply(\"tests/testdata/real_plant_analyzed/PointCloud_1_0_1_0_10_0_7ee836e5a9/PointCloud.ply\")\n    &gt;&gt;&gt; skel = load_json(\"tests/testdata/real_plant_analyzed/CurveSkeleton__TriangleMesh_0393cb5708/CurveSkeleton.json\", \"points\")\n    &gt;&gt;&gt; # Perform stochastic optimization\n    &gt;&gt;&gt; refined_skel = perform_registration(pcd, skel, alpha=5, beta=5)\n    &gt;&gt;&gt; print(skel.shape)\n    \"\"\"\n    # Add input point sets to kwargs to pass to DeformableRegistration\n    kwargs.update({'X': X, 'Y': Y})\n\n    # Initialize the Coherent Point Drift registration object\n    reg = DeformableRegistration(**kwargs)\n    # Perform registration\n    reg.register()\n\n    return reg.TY\n</code></pre>"},{"location":"reference/skeleton_refinement/utilities/","title":"utilities","text":""},{"location":"reference/skeleton_refinement/utilities/#skeleton_refinement.utilities--point-set-registration-utilities","title":"Point Set Registration Utilities","text":"<p>This module provides efficient mathematical functions to support point set registration algorithms, particularly for the Coherent Point Drift (CPD) method used in 3D point cloud alignment and deformable registration tasks.</p>"},{"location":"reference/skeleton_refinement/utilities/#skeleton_refinement.utilities--key-features","title":"Key Features","text":"<ul> <li>Variance initialization for point sets using efficient vectorized operations</li> <li>Gaussian kernel matrix computation for smooth spatial transformations</li> <li>Optimization for large point clouds through NumPy vectorization</li> <li>Support for N-dimensional point data (typically 2D or 3D spatial coordinates)</li> </ul>"},{"location":"reference/skeleton_refinement/utilities/#skeleton_refinement.utilities.estimate_sigma2_memory","title":"estimate_sigma2_memory","text":"<pre><code>estimate_sigma2_memory(X, Y)\n</code></pre> <p>Calculate the estimated memory usage for sigma squared computation.</p> <p>This function estimates the required memory for the computation of sigma squared based on given input arrays. The memory estimation accounts for the creation of intermediate arrays (<code>XX</code>, <code>YY</code>, <code>diff</code>, and <code>err</code>), which are simultaneously needed during computations.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The first input array of shape (N, D), where N is the number of samples and D is the number of dimensions.</p> required <code>Y</code> <code>ndarray</code> <p>The second input array of shape (M, _), where M is the number of samples.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The estimated number of bytes required for the computation of sigma squared.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from skeleton_refinement.utilities import estimate_sigma2_memory\n&gt;&gt;&gt; from skeleton_refinement.utilities import human_readable_size\n&gt;&gt;&gt; # Create large point sets\n&gt;&gt;&gt; X = np.random.rand(100000, 3)  # a point-cloud of 100,000 points in 3D\n&gt;&gt;&gt; Y = np.random.rand(1000, 3)  # a skeleton of 1,000 points in 3D\n&gt;&gt;&gt; req_mem = estimate_sigma2_memory(X, Y)\n&gt;&gt;&gt; print(req_mem)\n9600000000\n&gt;&gt;&gt; print(human_readable_size(req_mem))\n8.94 GB\n</code></pre> Source code in <code>skeleton_refinement/utilities.py</code> <pre><code>def estimate_sigma2_memory(X, Y):\n    \"\"\"Calculate the estimated memory usage for sigma squared computation.\n\n    This function estimates the required memory for the computation of sigma squared\n    based on given input arrays. The memory estimation accounts for the creation of\n    intermediate arrays (`XX`, `YY`, `diff`, and `err`), which are simultaneously\n    needed during computations.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        The first input array of shape (N, D), where N is the number of samples and\n        D is the number of dimensions.\n    Y : numpy.ndarray\n        The second input array of shape (M, _), where M is the number of samples.\n\n    Returns\n    -------\n    int\n        The estimated number of bytes required for the computation of sigma squared.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from skeleton_refinement.utilities import estimate_sigma2_memory\n    &gt;&gt;&gt; from skeleton_refinement.utilities import human_readable_size\n    &gt;&gt;&gt; # Create large point sets\n    &gt;&gt;&gt; X = np.random.rand(100000, 3)  # a point-cloud of 100,000 points in 3D\n    &gt;&gt;&gt; Y = np.random.rand(1000, 3)  # a skeleton of 1,000 points in 3D\n    &gt;&gt;&gt; req_mem = estimate_sigma2_memory(X, Y)\n    &gt;&gt;&gt; print(req_mem)\n    9600000000\n    &gt;&gt;&gt; print(human_readable_size(req_mem))\n    8.94 GB\n    \"\"\"\n    N, D = X.shape\n    M, _ = Y.shape\n    dtype_byte = X.itemsize\n    # Multiply by 4 since we need to create 4 arrays (`XX`, `YY`, `diff` &amp; `err`)\n    n_bytes = dtype_byte * M * N * D * 4\n    return n_bytes\n</code></pre>"},{"location":"reference/skeleton_refinement/utilities/#skeleton_refinement.utilities.gaussian_kernel","title":"gaussian_kernel","text":"<pre><code>gaussian_kernel(Y, beta)\n</code></pre> <p>Compute the Gaussian kernel matrix for a point set.</p> <p>This function calculates a Gaussian kernel matrix (Gram matrix) where each element <code>(i,j)</code> represents the Gaussian radial basis function between points <code>Y[i]</code> and <code>Y[j]</code>. The kernel is used in deformable point set registration algorithms to define smooth spatial transformations.</p> <p>Parameters:</p> Name Type Description Default <code>Y</code> <code>ndarray</code> <p>Point set of shape <code>(M, D)</code> where <code>M</code> is the number of points and <code>D</code> is the dimensionality of each point (typically 2 or 3 for spatial coordinates).</p> required <code>beta</code> <code>float</code> <p>The width parameter controlling the spatial scale of the Gaussian kernel. Larger values result in smoother transformations but less accurate registrations.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Gaussian kernel matrix of shape <code>(M, M)</code>, where each element <code>(i,j)</code> is: <code>exp(-||Y[i] - Y[j]||^2 / (2 * beta))</code></p> Notes <p>This function is typically used in deformable registration algorithms like Coherent Point Drift (CPD) to define smooth transformations between point sets.</p> <p>The implementation uses broadcasting and tiling operations to efficiently compute all pairwise distances without explicit loops.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from skeleton_refinement.utilities import gaussian_kernel\n&gt;&gt;&gt; Y = np.array([[0, 0, 0], [1, 0, 0], [0, 1, 0]])  # 3 points in 3D\n&gt;&gt;&gt; beta = 1.0\n&gt;&gt;&gt; G = gaussian_kernel(Y, beta)\n&gt;&gt;&gt; print(G.shape)\n(3, 3)\n&gt;&gt;&gt; print(np.round(G, 3))\n[[1.    0.607 0.607]\n [0.607 1.    0.368]\n [0.607 0.368 1.   ]]\n</code></pre> Source code in <code>skeleton_refinement/utilities.py</code> <pre><code>def gaussian_kernel(Y, beta):\n    \"\"\"Compute the Gaussian kernel matrix for a point set.\n\n    This function calculates a Gaussian kernel matrix (Gram matrix) where each element\n    ``(i,j)`` represents the Gaussian radial basis function between points ``Y[i]`` and ``Y[j]``.\n    The kernel is used in deformable point set registration algorithms to define\n    smooth spatial transformations.\n\n    Parameters\n    ----------\n    Y : numpy.ndarray\n        Point set of shape ``(M, D)`` where ``M`` is the number of points and\n        ``D`` is the dimensionality of each point (typically 2 or 3 for spatial coordinates).\n    beta : float\n        The width parameter controlling the spatial scale of the Gaussian kernel.\n        Larger values result in smoother transformations but less accurate registrations.\n\n    Returns\n    -------\n    numpy.ndarray\n        Gaussian kernel matrix of shape ``(M, M)``, where each element ``(i,j)`` is:\n        ``exp(-||Y[i] - Y[j]||^2 / (2 * beta))``\n\n    Notes\n    -----\n    This function is typically used in deformable registration algorithms like\n    Coherent Point Drift (CPD) to define smooth transformations between point sets.\n\n    The implementation uses broadcasting and tiling operations to efficiently\n    compute all pairwise distances without explicit loops.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from skeleton_refinement.utilities import gaussian_kernel\n    &gt;&gt;&gt; Y = np.array([[0, 0, 0], [1, 0, 0], [0, 1, 0]])  # 3 points in 3D\n    &gt;&gt;&gt; beta = 1.0\n    &gt;&gt;&gt; G = gaussian_kernel(Y, beta)\n    &gt;&gt;&gt; print(G.shape)\n    (3, 3)\n    &gt;&gt;&gt; print(np.round(G, 3))\n    [[1.    0.607 0.607]\n     [0.607 1.    0.368]\n     [0.607 0.368 1.   ]]\n    \"\"\"\n    # Extract dimensions from input point set\n    (M, D) = Y.shape  # M = number of points, D = dimensions per point\n\n    # Reshape Y into a 3D tensor with shape (1, M, D) for broadcasting\n    XX = np.reshape(Y, (1, M, D))\n    # Reshape Y into a 3D tensor with shape (M, 1, D) for broadcasting\n    YY = np.reshape(Y, (M, 1, D))\n    # Tile XX to create a tensor of shape (M, M, D) where each \"row\" is repeated M times\n    XX = np.tile(XX, (M, 1, 1))\n    # Tile YY to create a tensor of shape (M, M, D) where each \"column\" is repeated M times\n    YY = np.tile(YY, (1, M, 1))\n\n    # Calculate the element-wise difference between each pair of points\n    diff = XX - YY\n    # Square the differences\n    diff = np.multiply(diff, diff)\n    # Sum along the dimension axis to get squared Euclidean distances between each pair of points\n    diff = np.sum(diff, 2)\n\n    # Apply Gaussian RBF to get the kernel matrix: exp(-||Y[i] - Y[j]||^2 / (2 * beta))\n    return np.exp(-diff / (2 * beta))\n</code></pre>"},{"location":"reference/skeleton_refinement/utilities/#skeleton_refinement.utilities.get_available_memory","title":"get_available_memory","text":"<pre><code>get_available_memory()\n</code></pre> <p>Gets the amount of available physical memory in bytes using the <code>psutil</code> library.</p> <p>This function utilizes the <code>psutil</code> library to retrieve the current available physical memory on the system. It returns the number of bytes of memory that can be allocated for new or existing processes without causing pagefile swapping.</p> <p>Returns:</p> Type Description <code>int</code> <p>The amount of available memory in bytes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from skeleton_refinement.utilities import get_available_memory\n&gt;&gt;&gt; from skeleton_refinement.utilities import human_readable_size\n&gt;&gt;&gt; free_mem = get_available_memory()\n&gt;&gt;&gt; print(free_mem)\n7247376384\n&gt;&gt;&gt; print(human_readable_size(free_mem))\n6.75 GB\n</code></pre> Source code in <code>skeleton_refinement/utilities.py</code> <pre><code>def get_available_memory():\n    \"\"\"\n    Gets the amount of available physical memory in bytes using the `psutil` library.\n\n    This function utilizes the `psutil` library to retrieve the current available physical\n    memory on the system. It returns the number of bytes of memory that can be allocated\n    for new or existing processes without causing pagefile swapping.\n\n    Returns\n    -------\n    int\n        The amount of available memory in bytes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from skeleton_refinement.utilities import get_available_memory\n    &gt;&gt;&gt; from skeleton_refinement.utilities import human_readable_size\n    &gt;&gt;&gt; free_mem = get_available_memory()\n    &gt;&gt;&gt; print(free_mem)\n    7247376384\n    &gt;&gt;&gt; print(human_readable_size(free_mem))\n    6.75 GB\n    \"\"\"\n    import psutil\n    return psutil.virtual_memory().available\n</code></pre>"},{"location":"reference/skeleton_refinement/utilities/#skeleton_refinement.utilities.human_readable_size","title":"human_readable_size","text":"<pre><code>human_readable_size(bytes_value)\n</code></pre> <p>Convert a size in bytes to a human-readable string with appropriate unit.</p> <p>Parameters:</p> Name Type Description Default <code>bytes_value</code> <code>int</code> <p>Size in bytes</p> required <p>Returns:</p> Type Description <code>str</code> <p>Human-readable string with size and unit (B, KB, MB, GB, TB)</p> Source code in <code>skeleton_refinement/utilities.py</code> <pre><code>def human_readable_size(bytes_value):\n    \"\"\"Convert a size in bytes to a human-readable string with appropriate unit.\n\n    Parameters\n    ----------\n    bytes_value : int\n        Size in bytes\n\n    Returns\n    -------\n    str\n        Human-readable string with size and unit (B, KB, MB, GB, TB)\n    \"\"\"\n    # Define size units\n    units = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n\n    # Use logarithmic approach to determine the appropriate unit\n    if bytes_value == 0:\n        return \"0 B\"\n\n    # Find the appropriate unit\n    unit_index = 0\n    while bytes_value &gt;= 1024 and unit_index &lt; len(units) - 1:\n        bytes_value /= 1024\n        unit_index += 1\n\n    # Format with 2 decimal places if not in bytes\n    if unit_index == 0:\n        return f\"{int(bytes_value)} {units[unit_index]}\"\n    else:\n        return f\"{bytes_value:.2f} {units[unit_index]}\"\n</code></pre>"},{"location":"reference/skeleton_refinement/utilities/#skeleton_refinement.utilities.initialize_sigma2","title":"initialize_sigma2","text":"<pre><code>initialize_sigma2(X, Y, batch_size=1000)\n</code></pre> <p>Initialize the variance parameter for point set registration algorithms.</p> <p>Selects and initializes the appropriate method for computing <code>sigma2</code> based on the available system memory and the estimated memory requirement for processing input data. Switches to a memory-efficient method if the required memory exceeds the available system memory.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input data, typically representing the feature matrix.</p> required <code>Y</code> <code>array - like</code> <p>Target data, typically representing an outcome or dependent variable.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for the memory-efficient implementation. Defaults to <code>1000</code>.</p> <code>1000</code> <p>Returns:</p> Type Description <code>float</code> <p>The initial variance value, calculated as the mean squared distance between all point pairs from <code>X</code> and <code>Y</code>.</p> Source code in <code>skeleton_refinement/utilities.py</code> <pre><code>def initialize_sigma2(X, Y, batch_size=1000):\n    \"\"\"Initialize the variance parameter for point set registration algorithms.\n\n    Selects and initializes the appropriate method for computing `sigma2` based on\n    the available system memory and the estimated memory requirement for processing\n    input data. Switches to a memory-efficient method if the required memory\n    exceeds the available system memory.\n\n    Parameters\n    ----------\n    X : array-like\n        Input data, typically representing the feature matrix.\n    Y : array-like\n        Target data, typically representing an outcome or dependent variable.\n    batch_size : int, optional\n        Batch size for the memory-efficient implementation.\n        Defaults to ``1000``.\n\n    Returns\n    -------\n    float\n        The initial variance value, calculated as the mean squared distance\n        between all point pairs from `X` and `Y`.\n    \"\"\"\n    required_memory = estimate_sigma2_memory(X, Y)\n    available_memory = get_available_memory()\n    if required_memory &lt; available_memory:\n        return initialize_sigma2_fast(X, Y)\n    else:\n        return initialize_sigma2_mem_efficient(X, Y, batch_size=batch_size)\n</code></pre>"},{"location":"reference/skeleton_refinement/utilities/#skeleton_refinement.utilities.initialize_sigma2_fast","title":"initialize_sigma2_fast","text":"<pre><code>initialize_sigma2_fast(X, Y)\n</code></pre> <p>Initialize the variance parameter for point set registration algorithms.</p> <p>This function calculates the initial variance (sigma squared) between two point sets, which is used as a normalization factor in point set registration algorithms. It computes the mean squared distance between all pairs of points from sets X and Y.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>First point set of shape <code>(N, D)</code> where <code>N</code> is the number of points and <code>D</code> is the dimensionality of each point (typically 2 or 3 for spatial coordinates).</p> required <code>Y</code> <code>ndarray</code> <p>Second point set of shape <code>(M, D)</code> where <code>M</code> is the number of points and <code>D</code> is the dimensionality of each point (should match the dimensionality of X).</p> required <p>Returns:</p> Type Description <code>float</code> <p>The initial variance value, calculated as the mean squared distance between all point pairs from <code>X</code> and <code>Y</code>.</p> Notes <p>This function is typically used in point set registration algorithms like Coherent Point Drift (CPD) to initialize the variance parameter.</p> <p>The computation avoids explicit loops by using broadcasting and tiling operations to efficiently calculate distances between all point pairs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from skeleton_refinement.utilities import initialize_sigma2_fast\n&gt;&gt;&gt; X = np.array([[0, 0, 0], [1, 0, 0], [0, 1, 0]])  # 3 points in 3D\n&gt;&gt;&gt; Y = np.array([[0.1, 0.1, 0.1], [1.1, 0.1, 0.1]])  # 2 points in 3D\n&gt;&gt;&gt; sigma2 = initialize_sigma2(X, Y)\n&gt;&gt;&gt; print(f\"{sigma2:.6f}\")\n0.276667\n</code></pre> Source code in <code>skeleton_refinement/utilities.py</code> <pre><code>def initialize_sigma2_fast(X, Y):\n    \"\"\"Initialize the variance parameter for point set registration algorithms.\n\n    This function calculates the initial variance (sigma squared) between two point sets,\n    which is used as a normalization factor in point set registration algorithms.\n    It computes the mean squared distance between all pairs of points from sets X and Y.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        First point set of shape ``(N, D)`` where ``N`` is the number of points and\n        ``D`` is the dimensionality of each point (typically 2 or 3 for spatial coordinates).\n    Y : numpy.ndarray\n        Second point set of shape ``(M, D)`` where ``M`` is the number of points and\n        ``D`` is the dimensionality of each point (should match the dimensionality of X).\n\n    Returns\n    -------\n    float\n        The initial variance value, calculated as the mean squared distance\n        between all point pairs from `X` and `Y`.\n\n    Notes\n    -----\n    This function is typically used in point set registration algorithms like\n    Coherent Point Drift (CPD) to initialize the variance parameter.\n\n    The computation avoids explicit loops by using broadcasting and tiling operations\n    to efficiently calculate distances between all point pairs.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from skeleton_refinement.utilities import initialize_sigma2_fast\n    &gt;&gt;&gt; X = np.array([[0, 0, 0], [1, 0, 0], [0, 1, 0]])  # 3 points in 3D\n    &gt;&gt;&gt; Y = np.array([[0.1, 0.1, 0.1], [1.1, 0.1, 0.1]])  # 2 points in 3D\n    &gt;&gt;&gt; sigma2 = initialize_sigma2(X, Y)\n    &gt;&gt;&gt; print(f\"{sigma2:.6f}\")\n    0.276667\n    \"\"\"\n    # Get dimensions of the point sets\n    (N, D) = X.shape  # N = number of points in X, D = dimensionality of points\n    (M, _) = Y.shape  # M = number of points in Y\n\n    # Reshape X to add a dimension for broadcasting (1\u00d7N\u00d7D)\n    XX = np.reshape(X, (1, N, D))\n    # Reshape Y to add a dimension for broadcasting (M\u00d71\u00d7D)\n    YY = np.reshape(Y, (M, 1, D))\n    # Replicate XX along M dimension to create matrix of size (M\u00d7N\u00d7D)\n    XX = np.tile(XX, (M, 1, 1))\n    # Replicate YY along N dimension to create matrix of size (M\u00d7N\u00d7D)\n    YY = np.tile(YY, (1, N, 1))\n\n    # Calculate the difference between every pair of points\n    diff = XX - YY\n    # Square the differences\n    err = np.multiply(diff, diff)\n\n    # Return the mean squared distance between all point pairs\n    # Normalized by dimension D and total number of point pairs (M*N)\n    return np.sum(err) / (D * M * N)\n</code></pre>"},{"location":"reference/skeleton_refinement/utilities/#skeleton_refinement.utilities.initialize_sigma2_mem_efficient","title":"initialize_sigma2_mem_efficient","text":"<pre><code>initialize_sigma2_mem_efficient(X, Y, batch_size=1000)\n</code></pre> <p>Memory-efficient version of initialize_sigma2 that processes the point cloud in batches.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>First point set of shape <code>(N, D)</code> where <code>N</code> is the number of point-cloud points and <code>D</code> is the dimensionality of each point.</p> required <code>Y</code> <code>ndarray</code> <p>Second point set of shape <code>(M, D)</code> where <code>M</code> is the number of skeleton points and <code>D</code> is the dimensionality of each point.</p> required <code>batch_size</code> <code>int</code> <p>The size of the batch to process at a time, that is, the number of point-cloud points. Defaults to <code>1000</code>.</p> <code>1000</code> <p>Returns:</p> Type Description <code>float</code> <p>The initial variance value, calculated as the mean squared distance between all point pairs from <code>X</code> and <code>Y</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from skeleton_refinement.utilities import initialize_sigma2_mem_efficient\n&gt;&gt;&gt; from skeleton_refinement.utilities import estimate_sigma2_memory\n&gt;&gt;&gt; # Create sample point sets\n&gt;&gt;&gt; X = np.array([[0, 0, 0], [1, 0, 0], [0, 1, 0]])  # 3 points in 3D\n&gt;&gt;&gt; Y = np.array([[0.1, 0.1, 0.1], [1.1, 0.1, 0.1]])  # 2 points in 3D\n&gt;&gt;&gt; sigma2 = initialize_sigma2_mem_efficient(X, Y)\n&gt;&gt;&gt; print(f\"{sigma2:.6f}\")\n0.276667\n&gt;&gt;&gt; # Create large point sets\n&gt;&gt;&gt; X = np.random.rand(100000, 3)  # a point-cloud of 100,000 points in 3D\n&gt;&gt;&gt; Y = np.random.rand(1000, 3)  # a skeleton of 1,000 points in 3D\n&gt;&gt;&gt; sigma2 = initialize_sigma2_mem_efficient(X, Y)\n&gt;&gt;&gt; print(f\"{sigma2:.6f}\")\n0.165825\n</code></pre> Source code in <code>skeleton_refinement/utilities.py</code> <pre><code>def initialize_sigma2_mem_efficient(X, Y, batch_size=1000):\n    \"\"\"Memory-efficient version of initialize_sigma2 that processes the point cloud in batches.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        First point set of shape ``(N, D)`` where ``N`` is the number of point-cloud points and\n        ``D`` is the dimensionality of each point.\n    Y : numpy.ndarray\n        Second point set of shape ``(M, D)`` where ``M`` is the number of skeleton points and\n        ``D`` is the dimensionality of each point.\n    batch_size : int, optional\n        The size of the batch to process at a time, that is, the number of point-cloud points.\n        Defaults to ``1000``.\n\n    Returns\n    -------\n    float\n        The initial variance value, calculated as the mean squared distance\n        between all point pairs from `X` and `Y`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from skeleton_refinement.utilities import initialize_sigma2_mem_efficient\n    &gt;&gt;&gt; from skeleton_refinement.utilities import estimate_sigma2_memory\n    &gt;&gt;&gt; # Create sample point sets\n    &gt;&gt;&gt; X = np.array([[0, 0, 0], [1, 0, 0], [0, 1, 0]])  # 3 points in 3D\n    &gt;&gt;&gt; Y = np.array([[0.1, 0.1, 0.1], [1.1, 0.1, 0.1]])  # 2 points in 3D\n    &gt;&gt;&gt; sigma2 = initialize_sigma2_mem_efficient(X, Y)\n    &gt;&gt;&gt; print(f\"{sigma2:.6f}\")\n    0.276667\n    &gt;&gt;&gt; # Create large point sets\n    &gt;&gt;&gt; X = np.random.rand(100000, 3)  # a point-cloud of 100,000 points in 3D\n    &gt;&gt;&gt; Y = np.random.rand(1000, 3)  # a skeleton of 1,000 points in 3D\n    &gt;&gt;&gt; sigma2 = initialize_sigma2_mem_efficient(X, Y)\n    &gt;&gt;&gt; print(f\"{sigma2:.6f}\")\n    0.165825\n    \"\"\"\n    N, D = X.shape  # N = number of points in X, D = dimensionality of points\n    M, _ = Y.shape  # M = number of points in Y\n\n    # Initialize sum of squared differences\n    sum_sq_diff = 0.0\n\n    # Process X points in batches to reduce memory usage\n    batch_size = min(batch_size, N)  # Adjust batch size based on available memory\n\n    for i in range(0, N, batch_size):\n        batch_end = min(i + batch_size, N)\n        batch_X = X[i:batch_end]\n\n        # For each point in Y, compute distance to all points in the current X batch\n        for j in range(M):\n            # Get current Y point and reshape to (1, D) for broadcasting\n            y_point = Y[j].reshape(1, D)\n\n            # Calculate squared distances between all batch_X points and this Y point\n            diff = batch_X - y_point\n            sum_sq_diff += np.sum(diff * diff)\n\n    # Return the mean squared distance\n    return sum_sq_diff / (D * M * N)\n</code></pre>"},{"location":"reference/skeleton_refinement/cli/","title":"cli","text":""},{"location":"reference/skeleton_refinement/cli/refine_skeleton/","title":"refine_skeleton","text":""},{"location":"reference/skeleton_refinement/cli/refine_skeleton/#skeleton_refinement.cli.refine_skeleton--skeleton-refinement-tool","title":"Skeleton Refinement Tool","text":"<p>A command-line utility for refining plant skeletons by aligning coarse skeleton structures with 3D point cloud data using a stochastic optimization framework. This tool helps improve the quality of plant representation by pushing skeleton points toward their proper positions in the original point cloud.</p> <p>There are two parameters that control the quality of alignment, alpha and beta.</p>"},{"location":"reference/skeleton_refinement/cli/refine_skeleton/#skeleton_refinement.cli.refine_skeleton--usage-examples","title":"Usage Examples","text":"<pre><code># Basic usage with default parameters\n$ refine_skeleton path/to/pointcloud.ply path/to/skeleton.json refined_skeleton.json\n\n# Advanced usage with custom parameters\n$ refine_skeleton path/to/pointcloud.ply path/to/skeleton.json refined_skeleton.json --alpha 0.5 --beta 0.1 --knn_mst --n_nei 8\n</code></pre>"},{"location":"reference/skeleton_refinement/cli/refine_skeleton/#skeleton_refinement.cli.refine_skeleton--author","title":"Author","text":"<p>Ayan Chaudhury, Inria team MOSAIC, Laboratoire Reproduction et D\u00e9veloppement des Plantes, Univ. Lyon, ENS de Lyon, UCB Lyon 1, CNRS, INRA, Inria France</p>"},{"location":"reference/skeleton_refinement/cli/refine_skeleton/#skeleton_refinement.cli.refine_skeleton.file_loader","title":"file_loader","text":"<pre><code>file_loader(fname)\n</code></pre> <p>Load point cloud or skeleton from a file.</p> Source code in <code>skeleton_refinement/cli/refine_skeleton.py</code> <pre><code>def file_loader(fname):\n    \"\"\"Load point cloud or skeleton from a file.\"\"\"\n    if fname.suffix in [\".xyz\", \".txt\"]:\n        xyz = load_xyz(fname)\n    elif fname.suffix == \".json\":\n        xyz = load_json(fname, \"points\")\n    elif fname.suffix == \".ply\":\n        xyz = load_ply(fname)\n    elif fname.suffix == \".p\":\n        xyz = load_nx(fname)\n    else:\n        raise IOError(f\"Unknown input file format '{fname.suffix}' for file '{fname}'! Choose from {IN_FMT}.\")\n    return xyz\n</code></pre>"},{"location":"reference/skeleton_refinement/cli/refine_skeleton/#skeleton_refinement.cli.refine_skeleton.file_writer","title":"file_writer","text":"<pre><code>file_writer(fname, skel)\n</code></pre> <p>Write skeleton to a file.</p> Source code in <code>skeleton_refinement/cli/refine_skeleton.py</code> <pre><code>def file_writer(fname, skel):\n    \"\"\"Write skeleton to a file.\"\"\"\n    if fname.suffix in [\".xyz\", \".txt\"]:\n        np.savetxt(fname, skel, delimiter=' ')\n    elif fname.suffix == \".json\":\n        save_json(fname, skel)\n    elif fname.suffix == \".p\":\n        save_nx(fname, skel)\n    else:\n        raise IOError(f\"Unknown output file format {fname.suffix}!\")\n    return\n</code></pre>"}]}